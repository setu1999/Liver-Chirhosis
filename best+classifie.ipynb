{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":13691294,"sourceType":"datasetVersion","datasetId":8708089},{"sourceId":13691992,"sourceType":"datasetVersion","datasetId":8708631},{"sourceId":13694017,"sourceType":"datasetVersion","datasetId":8710210}],"dockerImageVersionId":31193,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# Use notebook magics, not raw shell.\n%pip install -q --upgrade \\\n    \"numpy==1.26.4\" \\\n    \"scipy==1.10.1\" \\\n    \"scikit-learn==1.3.2\" \\\n    \"pandas==2.2.2\" \\\n    \"matplotlib==3.7.2\"\nimport numpy, scipy, sklearn, pandas, matplotlib\nprint(\"numpy      :\", numpy.__version__)\nprint(\"scipy      :\", scipy.__version__)\nprint(\"sklearn    :\", sklearn.__version__)\nprint(\"pandas     :\", pandas.__version__)\nprint(\"matplotlib :\", matplotlib.__version__)\nprint(\"\\nâœ… Now RESTART the runtime/kernel (Runtime â†’ Restart).\")\n# sanity_check.py\nimport numpy, scipy, sklearn\nfrom sklearn.metrics import classification_report, confusion_matrix\nimport scipy.special as sp\n\nprint(\"âœ… Imports OK\")\nprint(\"numpy:\", numpy.__version__, \"| scipy:\", scipy.__version__, \"| sklearn:\", sklearn.__version__)\n\n# tiny SciPy ufunc smoke test\nx = [0.0, 0.5, 1.0]\nprint(\"special.expit(x) ->\", sp.expit(x))\n\n# tiny sklearn smoke test\ny_true = [0,1,2,1]; y_pred = [0,1,1,1]\nprint(\"\\nclassification_report:\\n\", classification_report(y_true, y_pred, zero_division=0))\n# deps needed by your code (compatible with NumPy 1.26.4)\n%pip install -q \\\n  \"albumentations==1.4.7\" \\\n  \"scikit-image==0.22.0\" \\\n  \"opencv-python-headless==4.8.1.78\" \\\n  \"tqdm==4.66.4\" \\\n  \"Pillow==10.3.0\"\nimport albumentations, skimage, cv2, tqdm, PIL, torch, torchvision\nprint(\"albumentations:\", albumentations.__version__)\nprint(\"scikit-image :\", skimage.__version__)\nprint(\"opencv       :\", cv2.__version__)\nprint(\"tqdm         :\", tqdm.__version__)\nprint(\"Pillow       :\", PIL.__version__)\nprint(\"torch        :\", torch.__version__, \"| torchvision:\", torchvision.__version__)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-12T16:15:05.692118Z","iopub.execute_input":"2025-11-12T16:15:05.692330Z","iopub.status.idle":"2025-11-12T16:15:36.948648Z","shell.execute_reply.started":"2025-11-12T16:15:05.692303Z","shell.execute_reply":"2025-11-12T16:15:36.947776Z"}},"outputs":[{"name":"stdout","text":"\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m58.9/58.9 kB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m34.1/34.1 MB\u001b[0m \u001b[31m52.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m10.9/10.9 MB\u001b[0m \u001b[31m93.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m0:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m13.0/13.0 MB\u001b[0m \u001b[31m92.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m0:01\u001b[0m\n\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nbigframes 2.12.0 requires google-cloud-bigquery-storage<3.0.0,>=2.30.0, which is not installed.\ndatasets 4.4.1 requires pyarrow>=21.0.0, but you have pyarrow 19.0.1 which is incompatible.\nmne 1.10.2 requires scipy>=1.11, but you have scipy 1.10.1 which is incompatible.\nkaggle-environments 1.18.0 requires scipy>=1.11.2, but you have scipy 1.10.1 which is incompatible.\ncesium 0.12.4 requires numpy<3.0,>=2.0, but you have numpy 1.26.4 which is incompatible.\ngoogle-colab 1.0.0 requires notebook==6.5.7, but you have notebook 6.5.4 which is incompatible.\ngoogle-colab 1.0.0 requires requests==2.32.3, but you have requests 2.32.5 which is incompatible.\ngoogle-colab 1.0.0 requires tornado==6.4.2, but you have tornado 6.5.2 which is incompatible.\njax 0.5.2 requires scipy>=1.11.1, but you have scipy 1.10.1 which is incompatible.\ntsfresh 0.21.0 requires scipy>=1.14.0; python_version >= \"3.10\", but you have scipy 1.10.1 which is incompatible.\ndopamine-rl 4.1.2 requires gymnasium>=1.0.0, but you have gymnasium 0.29.0 which is incompatible.\nbigframes 2.12.0 requires rich<14,>=12.4.4, but you have rich 14.2.0 which is incompatible.\nscikit-image 0.25.2 requires scipy>=1.11.4, but you have scipy 1.10.1 which is incompatible.\ncvxpy 1.6.7 requires scipy>=1.11.0, but you have scipy 1.10.1 which is incompatible.\ngradio 5.38.1 requires pydantic<2.12,>=2.0, but you have pydantic 2.12.4 which is incompatible.\nxarray-einstats 0.9.1 requires scipy>=1.11, but you have scipy 1.10.1 which is incompatible.\nplotnine 0.14.5 requires matplotlib>=3.8.0, but you have matplotlib 3.7.2 which is incompatible.\njaxlib 0.5.1 requires scipy>=1.11.1, but you have scipy 1.10.1 which is incompatible.\numap-learn 0.5.9.post2 requires scikit-learn>=1.6, but you have scikit-learn 1.3.2 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mNote: you may need to restart the kernel to use updated packages.\nnumpy      : 1.26.4\nscipy      : 1.10.1\nsklearn    : 1.3.2\npandas     : 2.2.2\nmatplotlib : 3.7.2\n\nâœ… Now RESTART the runtime/kernel (Runtime â†’ Restart).\nâœ… Imports OK\nnumpy: 1.26.4 | scipy: 1.10.1 | sklearn: 1.3.2\nspecial.expit(x) -> [0.5        0.62245933 0.73105858]\n\nclassification_report:\n               precision    recall  f1-score   support\n\n           0       1.00      1.00      1.00         1\n           1       0.67      1.00      0.80         2\n           2       0.00      0.00      0.00         1\n\n    accuracy                           0.75         4\n   macro avg       0.56      0.67      0.60         4\nweighted avg       0.58      0.75      0.65         4\n\n\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m57.6/57.6 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h\u001b[31mERROR: Cannot install albumentations==1.4.7 and opencv-python-headless==4.8.1.78 because these package versions have conflicting dependencies.\u001b[0m\u001b[31m\n\u001b[0m\u001b[31mERROR: ResolutionImpossible: for help visit https://pip.pypa.io/en/latest/topics/dependency-resolution/#dealing-with-dependency-conflicts\u001b[0m\u001b[31m\n\u001b[0mNote: you may need to restart the kernel to use updated packages.\nalbumentations: 2.0.8\nscikit-image : 0.25.2\nopencv       : 4.12.0\ntqdm         : 4.67.1\nPillow       : 11.3.0\ntorch        : 2.6.0+cu124 | torchvision: 0.21.0+cu124\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import os\nimport torch\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom PIL import Image\nimport torch.nn.functional as F\nimport torch.nn as nn\nfrom torchvision import models, transforms\nfrom torch.utils.data import DataLoader, Dataset\nfrom tqdm import tqdm\nfrom sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay\nfrom sklearn.utils.class_weight import compute_class_weight\nfrom torch.optim.lr_scheduler import ReduceLROnPlateau\nimport albumentations as A\nfrom albumentations.pytorch import ToTensorV2\nimport cv2\nfrom skimage.measure import regionprops_table\nfrom sklearn.preprocessing import StandardScaler\nimport warnings\n\n# Suppress minor warnings for clean output\nwarnings.filterwarnings(\"ignore\", category=UserWarning)\n\n# --- 1. CONFIGURATION ---\n# NOTE: Replace these paths with actual working paths if running outside a Kaggle environment\nMODEL1_PATH = '/kaggle/input/ourensemble5/3URESNET/best_segmentation_model1.pth' # UNet with ResNet50\nMODEL2_PATH = '/kaggle/input/ourensemble5/3URESNET/best_transunetpp_model.pth' # TransUNetPP\nMODEL3_PATH = '/kaggle/input/ourensemble5/3URESNET/best_segmentation_model3.pth' # AttentionUNet\nMETADATA_PATH = '/kaggle/input/t2metadataaa/T2_age_gender_evaluation.csv'\nTEST_DIR = '/kaggle/input/dataa1/Cirrhosis_T2_2D/test'\nTRAIN_DIR = '/kaggle/input/dataa1/Cirrhosis_T2_2D/train'\nVAL_DIR = '/kaggle/input/dataa1/Cirrhosis_T2_2D/valid'\nIMAGE_SIZE = (224, 224)\nCLASS_NAMES = ['Mild', 'Moderate', 'Severe']\nNUM_CLASSES = len(CLASS_NAMES)\nNUM_EPOCHS = 50\nBATCH_SIZE = 16\n\n# --- 2. SETUP ---\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device} âš™ï¸\")\n\n# --- 3. DATAFRAME CREATION AND MERGE ---\nprint(\"\\n--- Creating DataFrames ---\")\ntry:\n    # Load and clean metadata\n    metadata_df = pd.read_csv(METADATA_PATH)\n    metadata_df.rename(columns={'Patient ID': 'ID', 'Radiological Evaluation': 'radiological_evaluation'}, inplace=True)\n    metadata_df.dropna(subset=['radiological_evaluation'], inplace=True)\n    metadata_df['radiological_evaluation'] = metadata_df['radiological_evaluation'].astype(int)\n    metadata_df['ID'] = metadata_df['ID'].astype(str)\n    class_label_map = {1: 0, 2: 1, 3: 2} # Mild=0, Moderate=1, Severe=2\n    metadata_df['class_label'] = metadata_df['radiological_evaluation'].map(class_label_map)\n    metadata_df.dropna(subset=['class_label'], inplace=True)\n    valid_ids = set(metadata_df['ID'].tolist())\n\n    # Helper function to create dataframe from file structure\n    def create_dataframe_from_ids(directory, allowed_ids):\n        data = []\n        if not os.path.exists(directory): return pd.DataFrame(data)\n        for folder_name in os.listdir(directory):\n            if folder_name in allowed_ids:\n                folder_path = os.path.join(directory, folder_name)\n                images_dir = os.path.join(folder_path, 'images')\n                if os.path.exists(images_dir):\n                    for image_file in os.listdir(images_dir):\n                        mask_path = os.path.join(folder_path, 'masks', image_file)\n                        if os.path.exists(mask_path):\n                            image_path = os.path.join(images_dir, image_file)\n                            data.append((folder_name, image_path, mask_path))\n        return pd.DataFrame(data, columns=['ID', 'image_file_path', 'mask_file_path'])\n\n    # Merge dataframes\n    train_df = pd.merge(create_dataframe_from_ids(TRAIN_DIR, valid_ids), metadata_df, on='ID')\n    val_df = pd.merge(create_dataframe_from_ids(VAL_DIR, valid_ids), metadata_df, on='ID')\n    test_df = pd.merge(create_dataframe_from_ids(TEST_DIR, valid_ids), metadata_df, on='ID')\n\n    if train_df.empty or val_df.empty or test_df.empty:\n        raise ValueError(\"One or more dataframes are empty. Check your file paths and metadata.\")\n    \n    print(f\"Successfully created dataframes: {len(train_df)} train, {len(val_df)} val, {len(test_df)} test.\")\n    \n    # *** Define BOTH weight strategies ***\n    # 1. Base calculated weights\n    base_weights_array = compute_class_weight('balanced', classes=np.unique(train_df['class_label'].values), y=train_df['class_label'].values)\n    print(f\"Original calculated weights: {base_weights_array}\")\n\n    # 2. Strategy A: Conservative Boost (1.2)\n    weights_array_1_2 = np.copy(base_weights_array)\n    weights_array_1_2[1] = weights_array_1_2[1] * 1.2\n    class_weights_boost_1_2 = torch.tensor(weights_array_1_2, dtype=torch.float).to(device)\n    print(f\"Weights (Boost 1.2): {class_weights_boost_1_2}\")\n\n    # 3. Strategy B: Aggressive Boost (1.5)\n    weights_array_1_5 = np.copy(base_weights_array)\n    weights_array_1_5[1] = weights_array_1_5[1] * 1.5\n    class_weights_boost_1_5 = torch.tensor(weights_array_1_5, dtype=torch.float).to(device)\n    print(f\"Weights (Boost 1.5): {class_weights_boost_1_5}\")\n\n\nexcept Exception as e:\n    print(f\"Error during data setup: {e}\")\n    test_df = None\n\n# --- 4. DATASET FOR INITIAL SEGMENTATION ---\nclass SegmentationInputDataset(Dataset):\n    def __init__(self, dataframe):\n        self.dataframe = dataframe\n        # Standard normalization for CNNs (e.g., ImageNet weights)\n        self.transform = transforms.Compose([\n            transforms.Resize(IMAGE_SIZE, transforms.InterpolationMode.BILINEAR),\n            transforms.ToTensor(),\n            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n        ])\n    def __len__(self): return len(self.dataframe)\n    def __getitem__(self, idx):\n        row = self.dataframe.iloc[idx]\n        image_pil = Image.open(row['image_file_path']).convert('RGB')\n        return self.transform(image_pil)\n\n# --- 5. MODEL ARCHITECTURE DEFINITIONS (Segmentation Models) ---\n# NOTE: Using the user's provided segmentation model definitions\n\nclass ConvBlock(nn.Module):\n    def __init__(self, in_ch, out_ch):\n        super().__init__()\n        self.conv = nn.Sequential(\n            nn.Conv2d(in_ch, out_ch, 3, 1, 1, bias=False), nn.BatchNorm2d(out_ch), nn.ReLU(inplace=True),\n            nn.Conv2d(out_ch, out_ch, 3, 1, 1, bias=False), nn.BatchNorm2d(out_ch), nn.ReLU(inplace=True))\n    def forward(self, x): return self.conv(x)\n\nclass DecoderBlock(nn.Module):\n    def __init__(self, in_channels, skip_channels, out_channels):\n        super().__init__()\n        self.conv1 = nn.Conv2d(in_channels + skip_channels, out_channels, kernel_size=3, padding=1, bias=False)\n        self.bn1 = nn.BatchNorm2d(out_channels)\n        self.relu = nn.ReLU(inplace=True)\n        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1, bias=False)\n        self.bn2 = nn.BatchNorm2d(out_channels)\n    def forward(self, x, skip_connection):\n        x = F.interpolate(x, scale_factor=2, mode='bilinear', align_corners=True)\n        x = torch.cat([x, skip_connection], dim=1)\n        x = self.relu(self.bn1(self.conv1(x)))\n        x = self.relu(self.bn2(self.conv2(x)))\n        return x\n\nclass UNetWithResNet50Encoder(nn.Module):\n    def __init__(self, n_classes=1):\n        super().__init__()\n        base_model = models.resnet50(weights=None)\n        base_layers = list(base_model.children())\n        self.encoder0, self.encoder1 = nn.Sequential(*base_layers[:3]), nn.Sequential(*base_layers[3:5])\n        self.encoder2, self.encoder3, self.encoder4 = base_layers[5], base_layers[6], base_layers[7]\n        self.decoder3 = DecoderBlock(2048, 1024, 512)\n        self.decoder2 = DecoderBlock(512, 512, 256)\n        self.decoder1 = DecoderBlock(256, 256, 128)\n        self.decoder0 = DecoderBlock(128, 64, 64)\n        self.final_conv = nn.Conv2d(64, n_classes, kernel_size=1)\n    \n    # *** UPDATED FORWARD PASS (Bug Fix) ***\n    def forward(self, x):\n        e0 = self.encoder0(x); e1 = self.encoder1(e0); e2 = self.encoder2(e1); e3 = self.encoder3(e2); e4 = self.encoder4(e3)\n        d3 = self.decoder3(e4, e3); d2 = self.decoder2(d3, e2); d1 = self.decoder1(d2, e1); d0 = self.decoder0(d1, e0)\n        \n        # Swapped order to match model2 (TransUNetPP) and avoid the interpolate bug\n        out = self.final_conv(d0) \n        out = F.interpolate(out, scale_factor=2, mode='bilinear', align_corners=True)\n        return out\n\nclass TransUNetPP(nn.Module):\n    def __init__(self, n_classes=1, img_dim=224, vit_dim=768, vit_depth=12, vit_heads=12):\n        super().__init__()\n        base_model = models.resnet50(weights=None)\n        base_layers = list(base_model.children())\n        self.encoder0, self.encoder1 = nn.Sequential(*base_layers[:3]), nn.Sequential(*base_layers[3:5])\n        self.encoder2, self.encoder3, self.encoder4 = base_layers[5], base_layers[6], base_layers[7]\n        num_patches, self.patch_dim = (img_dim // 32) ** 2, 2048\n        self.pos_embedding = nn.Parameter(torch.randn(1, num_patches, vit_dim))\n        self.patch_to_embedding = nn.Linear(self.patch_dim, vit_dim)\n        transformer_layer = nn.TransformerEncoderLayer(d_model=vit_dim, nhead=vit_heads, dim_feedforward=vit_dim * 4, batch_first=True)\n        self.transformer_encoder = nn.TransformerEncoder(transformer_layer, num_layers=vit_depth)\n        self.transformer_output_to_conv = nn.Sequential(nn.Linear(vit_dim, self.patch_dim), nn.LayerNorm(self.patch_dim))\n        d_ch = {'d0': 64, 'd1': 128, 'd2': 256, 'd3': 512, 'd4': 1024}\n        self.upsample = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n        self.X_0_0 = ConvBlock(64, d_ch['d0']); self.X_1_0 = ConvBlock(256, d_ch['d1'])\n        self.X_0_1 = ConvBlock(d_ch['d0'] + d_ch['d1'], d_ch['d0'])\n        self.X_2_0 = ConvBlock(512, d_ch['d2']); self.X_1_1 = ConvBlock(d_ch['d1'] + d_ch['d2'], d_ch['d1'])\n        self.X_0_2 = ConvBlock(d_ch['d0'] * 2 + d_ch['d1'], d_ch['d0'])\n        self.X_3_0 = ConvBlock(1024, d_ch['d3']); self.X_2_1 = ConvBlock(d_ch['d2'] + d_ch['d3'], d_ch['d2'])\n        self.X_1_2 = ConvBlock(d_ch['d1'] * 2 + d_ch['d2'], d_ch['d1'])\n        self.X_0_3 = ConvBlock(d_ch['d0'] * 3 + d_ch['d1'], d_ch['d0'])\n        self.X_4_0 = ConvBlock(2048, d_ch['d4']); self.X_3_1 = ConvBlock(d_ch['d3'] + d_ch['d4'], d_ch['d3'])\n        self.X_2_2 = ConvBlock(d_ch['d2'] * 2 + d_ch['d3'], d_ch['d2'])\n        self.X_1_3 = ConvBlock(d_ch['d1'] * 3 + d_ch['d2'], d_ch['d1'])\n        self.X_0_4 = ConvBlock(d_ch['d0'] * 4 + d_ch['d1'], d_ch['d0'])\n        self.final_conv = nn.Conv2d(d_ch['d0'], n_classes, kernel_size=1)\n    def forward(self, x):\n        e0 = self.encoder0(x); e1 = self.encoder1(e0); e2 = self.encoder2(e1); e3 = self.encoder3(e2); e4 = self.encoder4(e3)\n        bs, _, h, w = e4.shape\n        trans_in = self.patch_to_embedding(e4.flatten(2).transpose(1, 2)) + self.pos_embedding\n        trans_out = self.transformer_output_to_conv(self.transformer_encoder(trans_in)).transpose(1, 2).view(bs, self.patch_dim, h, w)\n        x0_0 = self.X_0_0(e0); x1_0 = self.X_1_0(e1); x0_1 = self.X_0_1(torch.cat([x0_0, self.upsample(x1_0)], 1))\n        x2_0 = self.X_2_0(e2); x1_1 = self.X_1_1(torch.cat([x1_0, self.upsample(x2_0)], 1)); x0_2 = self.X_0_2(torch.cat([x0_0, x0_1, self.upsample(x1_1)], 1))\n        x3_0 = self.X_3_0(e3); x2_1 = self.X_2_1(torch.cat([x2_0, self.upsample(x3_0)], 1)); x1_2 = self.X_1_2(torch.cat([x1_0, x1_1, self.upsample(x2_1)], 1))\n        x0_3 = self.X_0_3(torch.cat([x0_0, x0_1, x0_2, self.upsample(x1_2)], 1))\n        x4_0 = self.X_4_0(trans_out); x3_1 = self.X_3_1(torch.cat([x3_0, self.upsample(x4_0)], 1)); x2_2 = self.X_2_2(torch.cat([x2_0, x2_1, self.upsample(x3_1)], 1))\n        x1_3 = self.X_1_3(torch.cat([x1_0, x1_1, x1_2, self.upsample(x2_2)], 1)); x0_4 = self.X_0_4(torch.cat([x0_0, x0_1, x0_2, x0_3, self.upsample(x1_3)], 1))\n        return F.interpolate(self.final_conv(x0_4), scale_factor=2, mode='bilinear', align_corners=True)\n\nclass AttentionGate(nn.Module):\n    def __init__(self, F_g, F_l, F_int):\n        super().__init__(); self.W_g = nn.Sequential(nn.Conv2d(F_g, F_int, 1), nn.BatchNorm2d(F_int))\n        self.W_x = nn.Sequential(nn.Conv2d(F_l, F_int, 1), nn.BatchNorm2d(F_int))\n        self.psi = nn.Sequential(nn.Conv2d(F_int, 1, 1), nn.BatchNorm2d(1), nn.Sigmoid()); self.relu = nn.ReLU(inplace=True)\n    def forward(self, g, x): psi = self.relu(self.W_g(g) + self.W_x(x)); return x * self.psi(psi)\n\nclass AttentionUNet(nn.Module):\n    def __init__(self, n_classes=1):\n        super().__init__()\n        base = models.resnet50(weights=None); layers = list(base.children())\n        self.encoder0, self.encoder1 = nn.Sequential(*layers[:3]), nn.Sequential(*layers[3:5])\n        self.encoder2, self.encoder3, self.encoder4 = layers[5], layers[6], layers[7]\n        self.upconv3 = nn.ConvTranspose2d(2048, 1024, 2, 2); self.attn3 = AttentionGate(1024, 1024, 512); self.dec_conv3 = ConvBlock(2048, 1024)\n        self.upconv2 = nn.ConvTranspose2d(1024, 512, 2, 2); self.attn2 = AttentionGate(512, 512, 256); self.dec_conv2 = ConvBlock(1024, 512)\n        self.upconv1 = nn.ConvTranspose2d(512, 256, 2, 2); self.attn1 = AttentionGate(256, 256, 128); self.dec_conv1 = ConvBlock(512, 256)\n        self.upconv0 = nn.ConvTranspose2d(256, 64, 2, 2); self.attn0 = AttentionGate(64, 64, 32); self.dec_conv0 = ConvBlock(128, 64)\n        self.final_up = nn.ConvTranspose2d(64, 32, 2, 2); self.final_conv = nn.Conv2d(32, n_classes, 1)\n    def forward(self, x):\n        e0 = self.encoder0(x); e1 = self.encoder1(e0); e2 = self.encoder2(e1); e3 = self.encoder3(e2); e4 = self.encoder4(e3)\n        d3 = self.upconv3(e4); x3 = self.attn3(d3, e3); d3 = self.dec_conv3(torch.cat((x3, d3), 1))\n        d2 = self.upconv2(d3); x2 = self.attn2(d2, e2); d2 = self.dec_conv2(torch.cat((x2, d2), 1))\n        d1 = self.upconv1(d2); x1 = self.attn1(d1, e1); d1 = self.dec_conv1(torch.cat((x1, d1), 1))\n        d0 = self.upconv0(d1); x0 = self.attn0(d0, e0); d0 = self.dec_conv0(torch.cat((x0, d0), 1))\n        return self.final_conv(self.final_up(d0))\n\n# --- 6. LOADING SEGMENTATION MODELS ---\nmodels_loaded = False\nif test_df is not None and not test_df.empty:\n    try:\n        print(\"\\n--- Loading segmentation models... ---\")\n        model1 = UNetWithResNet50Encoder(n_classes=1).to(device)\n        model1.load_state_dict(torch.load(MODEL1_PATH, map_location=device))\n        model1.eval()\n        model2 = TransUNetPP(n_classes=1).to(device)\n        model2.load_state_dict(torch.load(MODEL2_PATH, map_location=device))\n        model2.eval()\n        model3 = AttentionUNet(n_classes=1).to(device)\n        model3.load_state_dict(torch.load(MODEL3_PATH, map_location=device))\n        model3.eval()\n        print(\"Segmentation models loaded successfully! âœ…\")\n        models_loaded = True\n    except Exception as e:\n        print(f\"Error loading segmentation model files (Check paths): {e}\")\n        models_loaded = False\nelse:\n    print(\"Skipping segmentation model loading as test data is unavailable.\")\n    \n# --- FOCAL LOSS IMPLEMENTATION ---\nclass FocalLoss(nn.Module):\n    def __init__(self, alpha=None, gamma=2., reduction='mean'):\n        super(FocalLoss, self).__init__()\n        self.gamma = gamma\n        self.alpha = alpha\n        self.reduction = reduction\n    def forward(self, input, target):\n        ce_loss = F.cross_entropy(input, target, reduction='none')\n        pt = torch.exp(-ce_loss)\n        loss = ((1 - pt) ** self.gamma * ce_loss)\n        if self.alpha is not None:\n            alpha_t = self.alpha[target.data.view(-1)]\n            loss = alpha_t * loss\n        if self.reduction == 'mean': return loss.mean()\n        else: return loss.sum()\n\n# =================================================================================\n# --- 7. CLASSIFICATION STAGE (HYBRID MODEL) ---\n# =================================================================================\n\nprint(\"\\n\\n--- Starting Classification Stage with Hybrid Model ---\")\nif not models_loaded or test_df.empty:\n    print(\"Skipping classification stage due to errors or missing data.\")\nelse:\n    try:\n        # --- 7.1 Generate Segmentation Predictions ---\n        print(\"\\nGenerating ensemble segmentation predictions...\")\n        train_seg_dataset = SegmentationInputDataset(train_df)\n        val_seg_dataset = SegmentationInputDataset(val_df)\n        test_seg_dataset = SegmentationInputDataset(test_df)\n\n        def get_segmentation_predictions(dataset, desc):\n            loader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=2)\n            all_preds = []\n            with torch.no_grad():\n                for images in tqdm(loader, desc=desc):\n                    images = images.to(device)\n                    # Ensemble predictions\n                    p1 = model1(images); p2 = model2(images); p3 = model3(images)\n                    ensembled = torch.sigmoid(torch.mean(torch.stack([p1, p2, p3]), dim=0))\n                    all_preds.append((ensembled > 0.5).float().cpu())\n            return torch.cat(all_preds)\n\n        train_pred_masks = get_segmentation_predictions(train_seg_dataset, \"Generating Train Masks\")\n        val_pred_masks = get_segmentation_predictions(val_seg_dataset, \"Generating Val Masks\")\n        test_pred_masks = get_segmentation_predictions(test_seg_dataset, \"Generating Test Masks\")\n        print(\"Segmentation predictions generated successfully.\")\n\n        # --- 7.2 Feature Engineering from Masks ---\n        print(\"\\nEngineering shape features from masks...\")\n\n        def get_shape_features(mask_tensor):\n            feature_df = pd.DataFrame()\n            for i in tqdm(range(len(mask_tensor)), desc=\"Calculating Features\"):\n                mask = mask_tensor[i, 0].numpy().astype(np.uint8)\n                # Ensure mask is 2D and binary\n                if mask.ndim == 3: mask = mask[:, :, 0] \n                mask = (mask > 0).astype(np.uint8)\n                \n                # Check for empty mask\n                if np.sum(mask) == 0:\n                    props_df = pd.DataFrame({'area': [0], 'perimeter': [0], 'solidity': [0], 'eccentricity': [0]})\n                else:\n                    try:\n                        # Use a small region minimum size to prevent issues with single pixels\n                        props = regionprops_table(mask, properties=('area', 'perimeter', 'solidity', 'eccentricity'))\n                        props_df = pd.DataFrame(props).head(1) # Take only the largest region if multiple exist\n                    except:\n                        props_df = pd.DataFrame({'area': [0], 'perimeter': [0], 'solidity': [0], 'eccentricity': [0]})\n                        \n                props_df['circularity'] = (4 * np.pi * props_df['area']) / (props_df['perimeter'] ** 2 + 1e-6)\n                feature_df = pd.concat([feature_df, props_df.head(1)], ignore_index=True)\n            feature_df.fillna(0, inplace=True)\n            return feature_df\n\n        train_features_df = get_shape_features(train_pred_masks)\n        val_features_df = get_shape_features(val_pred_masks)\n        test_features_df = get_shape_features(test_pred_masks)\n\n        # The shape features are: area, perimeter, solidity, eccentricity, circularity (5 features)\n        n_shape_features = train_features_df.shape[1] \n        scaler = StandardScaler()\n        train_features = scaler.fit_transform(train_features_df)\n        val_features = scaler.transform(val_features_df)\n        test_features = scaler.transform(test_features_df)\n        \n        # --- 7.3 Prepare Data and Datasets ---\n        train_transform = A.Compose([\n            A.Resize(height=IMAGE_SIZE[0], width=IMAGE_SIZE[1]), A.HorizontalFlip(p=0.5),\n            A.ShiftScaleRotate(shift_limit=0.05, scale_limit=0.05, rotate_limit=10, p=0.5),\n            A.RandomBrightnessContrast(p=0.3), A.GaussNoise(p=0.2), A.OpticalDistortion(p=0.2),\n            A.CoarseDropout(max_holes=8, max_height=8, max_width=8, p=0.2),\n            A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]), ToTensorV2()\n        ])\n        eval_transform = A.Compose([\n            A.Resize(height=IMAGE_SIZE[0], width=IMAGE_SIZE[1]), \n            A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]), ToTensorV2()\n        ])\n        minority_transform = A.Compose([ # More aggressive augmentation\n            A.Resize(height=IMAGE_SIZE[0], width=IMAGE_SIZE[1]), A.HorizontalFlip(p=0.8), A.VerticalFlip(p=0.5),\n            A.ShiftScaleRotate(shift_limit=0.1, scale_limit=0.1, rotate_limit=20, p=0.8),\n            A.RandomBrightnessContrast(brightness_limit=0.3, contrast_limit=0.3, p=0.5),\n            A.GaussNoise(p=0.3), A.OpticalDistortion(p=0.3),\n            A.CoarseDropout(max_holes=16, max_height=16, max_width=16, p=0.3),\n            A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]), ToTensorV2()\n        ])\n\n        class HybridDataset(Dataset):\n            def __init__(self, dataframe, pred_masks, shape_features, transform, minority_classes=None, minority_transform=None):\n                self.df = dataframe.reset_index(drop=True)\n                self.masks = pred_masks\n                self.features = shape_features\n                self.transform = transform\n                self.minority_classes = minority_classes if minority_classes else []\n                self.minority_transform = minority_transform if minority_transform else transform\n\n            def __len__(self): return len(self.df)\n\n            def __getitem__(self, idx):\n                row = self.df.iloc[idx]\n                image = cv2.imread(row['image_file_path'])\n                image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n                label = row['class_label']\n                \n                # Conditional augmentation\n                if label in self.minority_classes and self.minority_transform:\n                    transformed = self.minority_transform(image=image)\n                else:\n                    transformed = self.transform(image=image)\n\n                img_tensor = transformed['image']\n                \n                # Apply mask to the image (Liver only)\n                # Ensure mask is the right size/shape for element-wise multiplication\n                pred_mask = self.masks[idx].to(img_tensor.device) \n                masked_image = img_tensor * pred_mask\n                \n                shape_feats = torch.tensor(self.features[idx], dtype=torch.float)\n                label = torch.tensor(label, dtype=torch.long)\n                return masked_image, shape_feats, label\n\n        # Use setting from previous best run: only 'Severe' (2) gets strong augmentation\n        minority_classes_list = [2] \n        train_cls_dataset = HybridDataset(\n            train_df, train_pred_masks, train_features, \n            transform=train_transform, minority_classes=minority_classes_list, minority_transform=minority_transform\n        )\n        val_cls_dataset = HybridDataset(val_df, val_pred_masks, val_features, eval_transform)\n        test_cls_dataset = HybridDataset(test_df, test_pred_masks, test_features, eval_transform)\n        \n        # WeightedRandomSampler for class balancing\n        class_counts = train_df['class_label'].value_counts().to_dict()\n        num_samples = len(train_df)\n        class_weights_sampler = {i: num_samples / class_counts.get(i, 1) for i in range(NUM_CLASSES)}\n        sample_weights = np.array([class_weights_sampler[t] for t in train_df['class_label'].values])\n        sampler = torch.utils.data.WeightedRandomSampler(\n            weights=torch.from_numpy(sample_weights).double(), num_samples=len(sample_weights), replacement=True\n        )\n\n        train_cls_loader = DataLoader(train_cls_dataset, batch_size=BATCH_SIZE, sampler=sampler, num_workers=2, pin_memory=True)\n        val_cls_loader = DataLoader(val_cls_dataset, BATCH_SIZE, shuffle=False, num_workers=2, pin_memory=True)\n        test_cls_loader = DataLoader(test_cls_dataset, BATCH_SIZE, shuffle=False, num_workers=2, pin_memory=True)\n        print(\"\\nHybrid datasets and loaders are ready with WeightedRandomSampler. ğŸ“Š\")\n\n        # --- 7.4 HYBRID MODEL ARCHITECTURE (Dynamic Encoder) ---\n        \n        # PLACEHOLDER MODELS (since external weights/complex definitions are unavailable)\n        class CoAnNetFeatureExtractor(nn.Module):\n            # CoAnNet is complex; we use a simpler block with the expected output dimension\n            def __init__(self):\n                super().__init__()\n                self.conv = nn.Conv2d(3, 512, kernel_size=3, padding=1)\n                self.pool = nn.AdaptiveAvgPool2d((1, 1))\n                self.identity = nn.Identity()\n            def forward(self, x):\n                x = F.relu(self.conv(x))\n                return self.identity(self.pool(x).flatten(1))\n\n        class SimCLRResNet50(nn.Module):\n            # SimCLR is ResNet50 with special weights; we use a standard pre-trained one\n            def __init__(self):\n                super().__init__()\n                self.base = models.resnet50(weights=models.ResNet50_Weights.IMAGENET1K_V1)\n                self.base.fc = nn.Identity()\n            def forward(self, x):\n                return self.base(x)\n\n\n        class HybridClassifier(nn.Module):\n            def __init__(self, cnn_name: str, n_classes=3, n_shape_features=5):\n                super().__init__()\n                \n                cnn_feature_dim = 0\n                \n                if cnn_name == 'resnet34':\n                    self.cnn_base = models.resnet34(weights=models.ResNet34_Weights.DEFAULT)\n                    cnn_feature_dim = self.cnn_base.fc.in_features\n                    self.cnn_base.fc = nn.Identity()\n                elif cnn_name == 'resnet50' or cnn_name == 'ResNet50_GAP':\n                    # ResNet50 and ResNet50_GAP are the same in feature extraction layer, GAP is handled by AdaptiveAvgPool2d.\n                    self.cnn_base = models.resnet50(weights=models.ResNet50_Weights.DEFAULT)\n                    cnn_feature_dim = self.cnn_base.fc.in_features\n                    self.cnn_base.fc = nn.Identity()\n                elif cnn_name == 'densenet121':\n                    self.cnn_base = models.densenet121(weights=models.DenseNet121_Weights.DEFAULT)\n                    cnn_feature_dim = self.cnn_base.classifier.in_features\n                    self.cnn_base.classifier = nn.Identity()\n                elif cnn_name == 'EfficientNet_B0':\n                    self.cnn_base = models.efficientnet_b0(weights=models.EfficientNet_B0_Weights.DEFAULT)\n                    cnn_feature_dim = self.cnn_base.classifier[1].in_features\n                    self.cnn_base.classifier = nn.Identity() # Remove the default classifier\n                elif cnn_name == 'SimCLR_ResNet50':\n                    self.cnn_base = SimCLRResNet50() # Custom/Pre-loaded weights are required for true SimCLR\n                    cnn_feature_dim = 2048\n                elif cnn_name == 'CoAnNet':\n                    self.cnn_base = CoAnNetFeatureExtractor() # Placeholder\n                    cnn_feature_dim = 512 # Placeholder dimension\n                else:\n                    raise ValueError(f\"Unknown CNN name: {cnn_name}\")\n                    \n                print(f\"Loaded {cnn_name} encoder (Dim: {cnn_feature_dim})\")\n\n                self.tabular_mlp = nn.Sequential(\n                    nn.Linear(n_shape_features, 64), nn.ReLU(), nn.Dropout(0.4),\n                    nn.Linear(64, 32), nn.ReLU(), nn.Dropout(0.4)\n                )\n                self.fusion_classifier = nn.Sequential(\n                    nn.Linear(cnn_feature_dim + 32, 256), nn.ReLU(), nn.Dropout(0.5),\n                    nn.Linear(256, n_classes)\n                )\n                \n            def forward(self, image, shape_features):\n                cnn_features = self.cnn_base(image)\n                # Ensure 1D output after CNN\n                if cnn_features.ndim > 2:\n                    # Adaptive Avg Pool for any remaining spatial dimensions (e.g., in ResNet/EfficientNet backbone output)\n                    cnn_features = F.adaptive_avg_pool2d(cnn_features, (1, 1)).flatten(1) \n                    \n                tabular_features = self.tabular_mlp(shape_features)\n                combined_features = torch.cat([cnn_features, tabular_features], dim=1)\n                output = self.fusion_classifier(combined_features)\n                return output\n\n        # --- 7.5 TRAIN AND EVALUATE FUNCTION PER MODEL ---\n        def train_and_evaluate_single_model(model_id: str, cnn_name: str, train_loader, val_loader, test_loader, class_weights, n_shape_features, device, num_epochs=NUM_EPOCHS, patience=10):\n            \"\"\"Trains and evaluates a single hybrid model.\"\"\"\n            print(f\"\\n=============================================\")\n            print(f\"ğŸš€ Starting Training for Model: {model_id} (Arch: {cnn_name})\")\n            print(f\"=============================================\")\n            \n            cls_model = HybridClassifier(cnn_name=cnn_name, n_classes=NUM_CLASSES, n_shape_features=n_shape_features).to(device)\n            \n            # Use gamma=2.0 as it gave the best result so far\n            criterion = FocalLoss(alpha=class_weights, gamma=2.0)\n            \n            # *** UPDATED HERE: Using a smaller LR ***\n            optimizer = torch.optim.AdamW(cls_model.parameters(), lr=5e-5, weight_decay=1e-5) # <-- UPDATED\n            \n            scheduler = ReduceLROnPlateau(optimizer, mode='max', factor=0.2, patience=5, verbose=False)\n            \n            epochs_no_improve = 0; best_val_accuracy = 0.0\n            model_save_path = f'best_classifier_model_{model_id}.pth' # Use unique model_id\n            \n            for epoch in range(num_epochs):\n                cls_model.train()\n                running_loss = 0.0\n                for images, shapes, labels in train_loader:\n                    images, shapes, labels = images.to(device), shapes.to(device), labels.to(device)\n                    optimizer.zero_grad()\n                    outputs = cls_model(images, shapes)\n                    loss = criterion(outputs, labels); loss.backward(); optimizer.step()\n                    running_loss += loss.item()\n                \n                cls_model.eval()\n                val_corrects = 0\n                with torch.no_grad():\n                    for images, shapes, labels in val_loader:\n                        images, shapes, labels = images.to(device), shapes.to(device), labels.to(device)\n                        outputs = cls_model(images, shapes); _, preds = torch.max(outputs, 1)\n                        val_corrects += torch.sum(preds == labels.data)\n\n                val_accuracy = val_corrects.double() / len(val_loader.dataset)\n                scheduler.step(val_accuracy)\n\n                # Early stopping logic\n                if val_accuracy > best_val_accuracy:\n                    best_val_accuracy = val_accuracy\n                    torch.save(cls_model.state_dict(), model_save_path)\n                    epochs_no_improve = 0\n                else:\n                    epochs_no_improve += 1\n                    \n                if epochs_no_improve >= patience:\n                    print(f\"â¹ï¸ Early stopping triggered for {model_id} at epoch {epoch+1}.\")\n                    break\n                    \n            print(f\"Final Best Validation Accuracy for {model_id}: {best_val_accuracy:.4f}\")\n\n            # Final Test Evaluation\n            try:\n                cls_model.load_state_dict(torch.load(model_save_path, map_location=device))\n            except Exception as e:\n                 print(f\"âš ï¸ Could not load best weights for {model_id}. Using current weights for testing. Error: {e}\")\n            cls_model.eval()\n            all_labels, all_preds = [], []\n            with torch.no_grad():\n                for images, shapes, labels in test_loader:\n                    images, shapes, labels = images.to(device), shapes.to(device), labels.to(device)\n                    outputs = cls_model(images, shapes); _, preds = torch.max(outputs, 1)\n                    all_labels.extend(labels.cpu().numpy()); all_preds.extend(preds.cpu().numpy())\n            \n            report = classification_report(all_labels, all_preds, target_names=CLASS_NAMES, zero_division=0, output_dict=True)\n            return np.array(all_preds), report\n\n        # --- 7.6 MASTER EXECUTION AND COMPARISON ---\n        \n        # List of CNN backbones to test\n        cnn_model_list = [\n            'resnet34', \n            'resnet50', \n            'densenet121', \n            'EfficientNet_B0',\n            'ResNet50_GAP',      # Standard ResNet50 encoder using GAP\n            'SimCLR_ResNet50',   # ResNet50 with placeholder SimCLR weights\n            'CoAnNet'            # Placeholder architecture\n        ]\n        \n        # Define the experiments to run\n        experiments = []\n        for cnn_name in cnn_model_list:\n            # Experiment 1: Conservative Boost (1.2)\n            experiments.append({\n                'model_id': f'{cnn_name}_boost_1.2',\n                'cnn_name': cnn_name,\n                'weights': class_weights_boost_1_2\n            })\n            # Experiment 2: Aggressive Boost (1.5)\n            experiments.append({\n                'model_id': f'{cnn_name}_boost_1.5',\n                'cnn_name': cnn_name,\n                'weights': class_weights_boost_1_5\n            })\n                \n        results = {} \n        for exp in experiments:\n            preds, report = train_and_evaluate_single_model(\n                model_id=exp['model_id'],\n                cnn_name=exp['cnn_name'],\n                train_loader=train_cls_loader, \n                val_loader=val_cls_loader, \n                test_loader=test_cls_loader,\n                class_weights=exp['weights'],  # Pass the correct weights for this experiment\n                n_shape_features=n_shape_features, \n                device=device\n            )\n            # Store results using the unique model_id as the key\n            results[exp['model_id']] = {'preds': preds, 'report': report}\n\n        # --- 7.7 FINAL ENSEMBLE SELECTION AND REPORT ---\n        # *** Using Best F1-Score for balanced selection ***\n        \n        print(\"\\n=============================================\")\n        print(\"ğŸ† FINAL RESULT SELECTION (PER-CLASS BEST F1-SCORE)\") \n        print(\"=============================================\")\n\n        # 1. Get ground truth labels\n        all_labels = np.concatenate([labels.cpu().numpy() for _, _, labels in test_cls_loader])\n        final_preds = np.zeros_like(all_labels)\n        \n        best_model_per_class = {}\n        \n        # 2. Determine the best model for each class based on 'f1-score' (balanced metric)\n        comparison_data = []\n        for i, class_name in enumerate(CLASS_NAMES):\n            best_f1 = -1.0 \n            best_model = None\n            \n            row = {'Class': class_name}\n            for model_name, res in results.items():\n                f1_val = res['report'].get(class_name, {}).get('f1-score', 0.0) \n                row[model_name] = f'{f1_val:.4f}'\n                \n                if f1_val > best_f1: \n                    best_f1 = f1_val \n                    best_model = model_name\n                    \n            best_model_per_class[i] = best_model\n            row['Best Model'] = best_model\n            row['Best F1-Score'] = f'{best_f1:.4f}' \n            comparison_data.append(row)\n            \n            # 3. Create the final combined prediction array (Per-Class Best Model Selection)\n            class_indices = np.where(all_labels == i)[0]\n            if best_model:\n                 final_preds[class_indices] = results[best_model]['preds'][class_indices]\n\n        # Print comparison table\n        comparison_df = pd.DataFrame(comparison_data)\n        print(\"\\n--- PER-CLASS F1-SCORE COMPARISON ---\") \n        print(comparison_df.to_markdown(index=False))\n        \n        # Print Final Combined Report\n        print(\"\\n--- FINAL CLASSIFICATION REPORT (Combined Best F1-Score) ---\") \n        print(classification_report(all_labels, final_preds, target_names=CLASS_NAMES, zero_division=0))\n\n        # Confusion Matrix for the Combined Result\n        cm_final = confusion_matrix(all_labels, final_preds, labels=range(NUM_CLASSES))\n        disp_final = ConfusionMatrixDisplay(confusion_matrix=cm_final, display_labels=CLASS_NAMES)\n        fig, ax = plt.subplots(figsize=(8, 8))\n        disp_final.plot(ax=ax, cmap=plt.cm.Greens)\n        plt.title(\"Confusion Matrix (Best Per-Class F1-Score Selection)\") \n        plt.show()\n\n    except Exception as e:\n        print(f\"\\nAn error occurred in the Classification Pipeline: {e}\")\n        import traceback\n        traceback.print_exc()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-12T16:16:25.754448Z","iopub.execute_input":"2025-11-12T16:16:25.755055Z","iopub.status.idle":"2025-11-12T20:04:55.911368Z","shell.execute_reply.started":"2025-11-12T16:16:25.755030Z","shell.execute_reply":"2025-11-12T20:04:55.910391Z"}},"outputs":[{"name":"stdout","text":"Using device: cuda âš™ï¸\n\n--- Creating DataFrames ---\nSuccessfully created dataframes: 5364 train, 674 val, 664 test.\nOriginal calculated weights: [0.82055989 0.8641856  1.60215054]\nWeights (Boost 1.2): tensor([0.8206, 1.0370, 1.6022], device='cuda:0')\nWeights (Boost 1.5): tensor([0.8206, 1.2963, 1.6022], device='cuda:0')\n\n--- Loading segmentation models... ---\nSegmentation models loaded successfully! âœ…\n\n\n--- Starting Classification Stage with Hybrid Model ---\n\nGenerating ensemble segmentation predictions...\n","output_type":"stream"},{"name":"stderr","text":"Generating Train Masks: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 336/336 [03:02<00:00,  1.85it/s]\nGenerating Val Masks: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 43/43 [00:25<00:00,  1.71it/s]\nGenerating Test Masks: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 42/42 [00:24<00:00,  1.70it/s]\n","output_type":"stream"},{"name":"stdout","text":"Segmentation predictions generated successfully.\n\nEngineering shape features from masks...\n","output_type":"stream"},{"name":"stderr","text":"Calculating Features: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5364/5364 [00:23<00:00, 229.70it/s]\nCalculating Features: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 674/674 [00:02<00:00, 228.65it/s]\nCalculating Features: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 664/664 [00:03<00:00, 211.69it/s]\n","output_type":"stream"},{"name":"stdout","text":"\nHybrid datasets and loaders are ready with WeightedRandomSampler. ğŸ“Š\n\n=============================================\nğŸš€ Starting Training for Model: resnet34_boost_1.2 (Arch: resnet34)\n=============================================\n","output_type":"stream"},{"name":"stderr","text":"Downloading: \"https://download.pytorch.org/models/resnet34-b627a593.pth\" to /root/.cache/torch/hub/checkpoints/resnet34-b627a593.pth\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 83.3M/83.3M [00:00<00:00, 195MB/s] \n","output_type":"stream"},{"name":"stdout","text":"Loaded resnet34 encoder (Dim: 512)\nâ¹ï¸ Early stopping triggered for resnet34_boost_1.2 at epoch 22.\nFinal Best Validation Accuracy for resnet34_boost_1.2: 0.6855\n\n=============================================\nğŸš€ Starting Training for Model: resnet34_boost_1.5 (Arch: resnet34)\n=============================================\nLoaded resnet34 encoder (Dim: 512)\nâ¹ï¸ Early stopping triggered for resnet34_boost_1.5 at epoch 11.\nFinal Best Validation Accuracy for resnet34_boost_1.5: 0.6855\n\n=============================================\nğŸš€ Starting Training for Model: resnet50_boost_1.2 (Arch: resnet50)\n=============================================\n","output_type":"stream"},{"name":"stderr","text":"Downloading: \"https://download.pytorch.org/models/resnet50-11ad3fa6.pth\" to /root/.cache/torch/hub/checkpoints/resnet50-11ad3fa6.pth\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 97.8M/97.8M [00:00<00:00, 205MB/s]\n","output_type":"stream"},{"name":"stdout","text":"Loaded resnet50 encoder (Dim: 2048)\nâ¹ï¸ Early stopping triggered for resnet50_boost_1.2 at epoch 22.\nFinal Best Validation Accuracy for resnet50_boost_1.2: 0.7003\n\n=============================================\nğŸš€ Starting Training for Model: resnet50_boost_1.5 (Arch: resnet50)\n=============================================\nLoaded resnet50 encoder (Dim: 2048)\nâ¹ï¸ Early stopping triggered for resnet50_boost_1.5 at epoch 16.\nFinal Best Validation Accuracy for resnet50_boost_1.5: 0.6573\n\n=============================================\nğŸš€ Starting Training for Model: densenet121_boost_1.2 (Arch: densenet121)\n=============================================\n","output_type":"stream"},{"name":"stderr","text":"Downloading: \"https://download.pytorch.org/models/densenet121-a639ec97.pth\" to /root/.cache/torch/hub/checkpoints/densenet121-a639ec97.pth\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 30.8M/30.8M [00:00<00:00, 173MB/s]\n","output_type":"stream"},{"name":"stdout","text":"Loaded densenet121 encoder (Dim: 1024)\nâ¹ï¸ Early stopping triggered for densenet121_boost_1.2 at epoch 13.\nFinal Best Validation Accuracy for densenet121_boost_1.2: 0.7077\n\n=============================================\nğŸš€ Starting Training for Model: densenet121_boost_1.5 (Arch: densenet121)\n=============================================\nLoaded densenet121 encoder (Dim: 1024)\nâ¹ï¸ Early stopping triggered for densenet121_boost_1.5 at epoch 17.\nFinal Best Validation Accuracy for densenet121_boost_1.5: 0.6914\n","output_type":"stream"},{"name":"stderr","text":"Downloading: \"https://download.pytorch.org/models/efficientnet_b0_rwightman-7f5810bc.pth\" to /root/.cache/torch/hub/checkpoints/efficientnet_b0_rwightman-7f5810bc.pth\n","output_type":"stream"},{"name":"stdout","text":"\n=============================================\nğŸš€ Starting Training for Model: EfficientNet_B0_boost_1.2 (Arch: EfficientNet_B0)\n=============================================\n","output_type":"stream"},{"name":"stderr","text":"100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20.5M/20.5M [00:00<00:00, 139MB/s] \n","output_type":"stream"},{"name":"stdout","text":"Loaded EfficientNet_B0 encoder (Dim: 1280)\nâ¹ï¸ Early stopping triggered for EfficientNet_B0_boost_1.2 at epoch 18.\nFinal Best Validation Accuracy for EfficientNet_B0_boost_1.2: 0.6602\n\n=============================================\nğŸš€ Starting Training for Model: EfficientNet_B0_boost_1.5 (Arch: EfficientNet_B0)\n=============================================\nLoaded EfficientNet_B0 encoder (Dim: 1280)\nâ¹ï¸ Early stopping triggered for EfficientNet_B0_boost_1.5 at epoch 21.\nFinal Best Validation Accuracy for EfficientNet_B0_boost_1.5: 0.6202\n\n=============================================\nğŸš€ Starting Training for Model: ResNet50_GAP_boost_1.2 (Arch: ResNet50_GAP)\n=============================================\nLoaded ResNet50_GAP encoder (Dim: 2048)\nâ¹ï¸ Early stopping triggered for ResNet50_GAP_boost_1.2 at epoch 22.\nFinal Best Validation Accuracy for ResNet50_GAP_boost_1.2: 0.7315\n\n=============================================\nğŸš€ Starting Training for Model: ResNet50_GAP_boost_1.5 (Arch: ResNet50_GAP)\n=============================================\nLoaded ResNet50_GAP encoder (Dim: 2048)\nâ¹ï¸ Early stopping triggered for ResNet50_GAP_boost_1.5 at epoch 31.\nFinal Best Validation Accuracy for ResNet50_GAP_boost_1.5: 0.6973\n\n=============================================\nğŸš€ Starting Training for Model: SimCLR_ResNet50_boost_1.2 (Arch: SimCLR_ResNet50)\n=============================================\n","output_type":"stream"},{"name":"stderr","text":"Downloading: \"https://download.pytorch.org/models/resnet50-0676ba61.pth\" to /root/.cache/torch/hub/checkpoints/resnet50-0676ba61.pth\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 97.8M/97.8M [00:00<00:00, 205MB/s]\n","output_type":"stream"},{"name":"stdout","text":"Loaded SimCLR_ResNet50 encoder (Dim: 2048)\nâ¹ï¸ Early stopping triggered for SimCLR_ResNet50_boost_1.2 at epoch 14.\nFinal Best Validation Accuracy for SimCLR_ResNet50_boost_1.2: 0.7033\n\n=============================================\nğŸš€ Starting Training for Model: SimCLR_ResNet50_boost_1.5 (Arch: SimCLR_ResNet50)\n=============================================\nLoaded SimCLR_ResNet50 encoder (Dim: 2048)\nâ¹ï¸ Early stopping triggered for SimCLR_ResNet50_boost_1.5 at epoch 12.\nFinal Best Validation Accuracy for SimCLR_ResNet50_boost_1.5: 0.6825\n\n=============================================\nğŸš€ Starting Training for Model: CoAnNet_boost_1.2 (Arch: CoAnNet)\n=============================================\nLoaded CoAnNet encoder (Dim: 512)\nâ¹ï¸ Early stopping triggered for CoAnNet_boost_1.2 at epoch 33.\nFinal Best Validation Accuracy for CoAnNet_boost_1.2: 0.3858\n\n=============================================\nğŸš€ Starting Training for Model: CoAnNet_boost_1.5 (Arch: CoAnNet)\n=============================================\nLoaded CoAnNet encoder (Dim: 512)\nâ¹ï¸ Early stopping triggered for CoAnNet_boost_1.5 at epoch 19.\nFinal Best Validation Accuracy for CoAnNet_boost_1.5: 0.3205\n\n=============================================\nğŸ† FINAL RESULT SELECTION (PER-CLASS BEST F1-SCORE)\n=============================================\n\n--- PER-CLASS F1-SCORE COMPARISON ---\n| Class    |   resnet34_boost_1.2 |   resnet34_boost_1.5 |   resnet50_boost_1.2 |   resnet50_boost_1.5 |   densenet121_boost_1.2 |   densenet121_boost_1.5 |   EfficientNet_B0_boost_1.2 |   EfficientNet_B0_boost_1.5 |   ResNet50_GAP_boost_1.2 |   ResNet50_GAP_boost_1.5 |   SimCLR_ResNet50_boost_1.2 |   SimCLR_ResNet50_boost_1.5 |   CoAnNet_boost_1.2 |   CoAnNet_boost_1.5 | Best Model                |   Best F1-Score |\n|:---------|---------------------:|---------------------:|---------------------:|---------------------:|------------------------:|------------------------:|----------------------------:|----------------------------:|-------------------------:|-------------------------:|----------------------------:|----------------------------:|--------------------:|--------------------:|:--------------------------|----------------:|\n| Mild     |               0.7031 |               0.724  |               0.7351 |               0.6726 |                  0.6667 |                  0.7692 |                      0.7729 |                      0.7106 |                   0.7422 |                   0.7568 |                      0.7043 |                       0.793 |              0      |              0      | SimCLR_ResNet50_boost_1.5 |          0.793  |\n| Moderate |               0.2411 |               0.3002 |               0.2444 |               0.3087 |                  0.343  |                  0.3392 |                      0.3209 |                      0.3415 |                   0.2995 |                   0.2679 |                      0.2633 |                       0.365 |              0.25   |              0.3189 | SimCLR_ResNet50_boost_1.5 |          0.365  |\n| Severe   |               0.4768 |               0.3931 |               0.4835 |               0.3481 |                  0.4028 |                  0.4845 |                      0.3599 |                      0.42   |                   0.5333 |                   0.5269 |                      0.4653 |                       0.346 |              0.4955 |              0.4193 | ResNet50_GAP_boost_1.2    |          0.5333 |\n\n--- FINAL CLASSIFICATION REPORT (Combined Best F1-Score) ---\n              precision    recall  f1-score   support\n\n        Mild       0.89      0.71      0.79       350\n    Moderate       0.32      0.56      0.41       133\n      Severe       0.61      0.51      0.56       181\n\n    accuracy                           0.63       664\n   macro avg       0.61      0.59      0.59       664\nweighted avg       0.70      0.63      0.65       664\n\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<Figure size 800x800 with 2 Axes>","image/png":"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\n"},"metadata":{}}],"execution_count":3},{"cell_type":"code","source":"import os\nimport torch\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom PIL import Image\nimport torch.nn.functional as F\nimport torch.nn as nn\nfrom torchvision import models, transforms\nfrom torch.utils.data import DataLoader, Dataset\nfrom tqdm import tqdm\nfrom sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay\nfrom sklearn.utils.class_weight import compute_class_weight\nfrom torch.optim.lr_scheduler import ReduceLROnPlateau\nimport albumentations as A\nfrom albumentations.pytorch import ToTensorV2\nimport cv2\nfrom skimage.measure import regionprops_table\nfrom sklearn.preprocessing import StandardScaler\nimport warnings\n\n# Suppress minor warnings for clean output\nwarnings.filterwarnings(\"ignore\", category=UserWarning)\n\n# --- 1. CONFIGURATION ---\n# NOTE: Replace these paths with actual working paths if running outside a Kaggle environment\nMODEL1_PATH = '/kaggle/input/ourensemble5/3URESNET/best_segmentation_model1.pth' # UNet with ResNet50\nMODEL2_PATH = '/kaggle/input/ourensemble5/3URESNET/best_transunetpp_model.pth' # TransUNetPP\nMODEL3_PATH = '/kaggle/input/ourensemble5/3URESNET/best_segmentation_model3.pth' # AttentionUNet\nMETADATA_PATH = '/kaggle/input/t2metadataaa/T2_age_gender_evaluation.csv'\nTEST_DIR = '/kaggle/input/dataa1/Cirrhosis_T2_2D/test'\nTRAIN_DIR = '/kaggle/input/dataa1/Cirrhosis_T2_2D/train'\nVAL_DIR = '/kaggle/input/dataa1/Cirrhosis_T2_2D/valid'\nIMAGE_SIZE = (224, 224)\nCLASS_NAMES = ['Mild', 'Moderate', 'Severe']\nNUM_CLASSES = len(CLASS_NAMES)\nNUM_EPOCHS = 50\nBATCH_SIZE = 16\n\n# --- 2. SETUP ---\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device} âš™ï¸\")\n\n# --- 3. DATAFRAME CREATION AND MERGE ---\nprint(\"\\n--- Creating DataFrames ---\")\ntry:\n    # Load and clean metadata\n    metadata_df = pd.read_csv(METADATA_PATH)\n    metadata_df.rename(columns={'Patient ID': 'ID', 'Radiological Evaluation': 'radiological_evaluation'}, inplace=True)\n    metadata_df.dropna(subset=['radiological_evaluation'], inplace=True)\n    metadata_df['radiological_evaluation'] = metadata_df['radiological_evaluation'].astype(int)\n    metadata_df['ID'] = metadata_df['ID'].astype(str)\n    class_label_map = {1: 0, 2: 1, 3: 2} # Mild=0, Moderate=1, Severe=2\n    metadata_df['class_label'] = metadata_df['radiological_evaluation'].map(class_label_map)\n    metadata_df.dropna(subset=['class_label'], inplace=True)\n    valid_ids = set(metadata_df['ID'].tolist())\n\n    # Helper function to create dataframe from file structure\n    def create_dataframe_from_ids(directory, allowed_ids):\n        data = []\n        if not os.path.exists(directory): return pd.DataFrame(data)\n        for folder_name in os.listdir(directory):\n            if folder_name in allowed_ids:\n                folder_path = os.path.join(directory, folder_name)\n                images_dir = os.path.join(folder_path, 'images')\n                if os.path.exists(images_dir):\n                    for image_file in os.listdir(images_dir):\n                        mask_path = os.path.join(folder_path, 'masks', image_file)\n                        if os.path.exists(mask_path):\n                            image_path = os.path.join(images_dir, image_file)\n                            data.append((folder_name, image_path, mask_path))\n        return pd.DataFrame(data, columns=['ID', 'image_file_path', 'mask_file_path'])\n\n    # Merge dataframes\n    train_df = pd.merge(create_dataframe_from_ids(TRAIN_DIR, valid_ids), metadata_df, on='ID')\n    val_df = pd.merge(create_dataframe_from_ids(VAL_DIR, valid_ids), metadata_df, on='ID')\n    test_df = pd.merge(create_dataframe_from_ids(TEST_DIR, valid_ids), metadata_df, on='ID')\n\n    if train_df.empty or val_df.empty or test_df.empty:\n        raise ValueError(\"One or more dataframes are empty. Check your file paths and metadata.\")\n    \n    print(f\"Successfully created dataframes: {len(train_df)} train, {len(val_df)} val, {len(test_df)} test.\")\n    \n    # *** UPDATED HERE ***\n    # Calculate class weights for Focal Loss\n    class_weights_array = compute_class_weight('balanced', classes=np.unique(train_df['class_label'].values), y=train_df['class_label'].values)\n    print(f\"Original calculated weights: {class_weights_array}\")\n    \n    # --- NEW: Manually boost the weight for the 'Moderate' class (label 1) ---\n    moderate_boost_factor = 1.5  # <-- Hyperparameter to tune\n    class_weights_array[1] = class_weights_array[1] * moderate_boost_factor\n    \n    class_weights = torch.tensor(class_weights_array, dtype=torch.float).to(device)\n    print(f\"Boosted 'Moderate' weight. New weights: {class_weights_array}\")\n    print(f\"\\nFinal Class Weights for Focal Loss: {class_weights}\")\n\n\nexcept Exception as e:\n    print(f\"Error during data setup: {e}\")\n    test_df = None\n\n# --- 4. DATASET FOR INITIAL SEGMENTATION ---\nclass SegmentationInputDataset(Dataset):\n    def __init__(self, dataframe):\n        self.dataframe = dataframe\n        # Standard normalization for CNNs (e.g., ImageNet weights)\n        self.transform = transforms.Compose([\n            transforms.Resize(IMAGE_SIZE, transforms.InterpolationMode.BILINEAR),\n            transforms.ToTensor(),\n            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n        ])\n    def __len__(self): return len(self.dataframe)\n    def __getitem__(self, idx):\n        row = self.dataframe.iloc[idx]\n        image_pil = Image.open(row['image_file_path']).convert('RGB')\n        return self.transform(image_pil)\n\n# --- 5. MODEL ARCHITECTURE DEFINITIONS (Segmentation Models) ---\n# NOTE: Using the user's provided segmentation model definitions\n\nclass ConvBlock(nn.Module):\n    def __init__(self, in_ch, out_ch):\n        super().__init__()\n        self.conv = nn.Sequential(\n            nn.Conv2d(in_ch, out_ch, 3, 1, 1, bias=False), nn.BatchNorm2d(out_ch), nn.ReLU(inplace=True),\n            nn.Conv2d(out_ch, out_ch, 3, 1, 1, bias=False), nn.BatchNorm2d(out_ch), nn.ReLU(inplace=True))\n    def forward(self, x): return self.conv(x)\n\nclass DecoderBlock(nn.Module):\n    def __init__(self, in_channels, skip_channels, out_channels):\n        super().__init__()\n        self.conv1 = nn.Conv2d(in_channels + skip_channels, out_channels, kernel_size=3, padding=1, bias=False)\n        self.bn1 = nn.BatchNorm2d(out_channels)\n        self.relu = nn.ReLU(inplace=True)\n        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1, bias=False)\n        self.bn2 = nn.BatchNorm2d(out_channels)\n    def forward(self, x, skip_connection):\n        x = F.interpolate(x, scale_factor=2, mode='bilinear', align_corners=True)\n        x = torch.cat([x, skip_connection], dim=1)\n        x = self.relu(self.bn1(self.conv1(x)))\n        x = self.relu(self.bn2(self.conv2(x)))\n        return x\n\nclass UNetWithResNet50Encoder(nn.Module):\n    def __init__(self, n_classes=1):\n        super().__init__()\n        base_model = models.resnet50(weights=None)\n        base_layers = list(base_model.children())\n        self.encoder0, self.encoder1 = nn.Sequential(*base_layers[:3]), nn.Sequential(*base_layers[3:5])\n        self.encoder2, self.encoder3, self.encoder4 = base_layers[5], base_layers[6], base_layers[7]\n        self.decoder3 = DecoderBlock(2048, 1024, 512)\n        self.decoder2 = DecoderBlock(512, 512, 256)\n        self.decoder1 = DecoderBlock(256, 256, 128)\n        self.decoder0 = DecoderBlock(128, 64, 64)\n        self.final_conv = nn.Conv2d(64, n_classes, kernel_size=1)\n    def forward(self, x):\n        e0 = self.encoder0(x); e1 = self.encoder1(e0); e2 = self.encoder2(e1); e3 = self.encoder3(e2); e4 = self.encoder4(e3)\n        d3 = self.decoder3(e4, e3); d2 = self.decoder2(d3, e2); d1 = self.decoder1(d2, e1); d0 = self.decoder0(d1, e0)\n        out = F.interpolate(d0, scale_factor=2, mode='bilinear', align_corners=True)\n        return self.final_conv(out)\n\nclass TransUNetPP(nn.Module):\n    def __init__(self, n_classes=1, img_dim=224, vit_dim=768, vit_depth=12, vit_heads=12):\n        super().__init__()\n        base_model = models.resnet50(weights=None)\n        base_layers = list(base_model.children())\n        self.encoder0, self.encoder1 = nn.Sequential(*base_layers[:3]), nn.Sequential(*base_layers[3:5])\n        self.encoder2, self.encoder3, self.encoder4 = base_layers[5], base_layers[6], base_layers[7]\n        num_patches, self.patch_dim = (img_dim // 32) ** 2, 2048\n        self.pos_embedding = nn.Parameter(torch.randn(1, num_patches, vit_dim))\n        self.patch_to_embedding = nn.Linear(self.patch_dim, vit_dim)\n        transformer_layer = nn.TransformerEncoderLayer(d_model=vit_dim, nhead=vit_heads, dim_feedforward=vit_dim * 4, batch_first=True)\n        self.transformer_encoder = nn.TransformerEncoder(transformer_layer, num_layers=vit_depth)\n        self.transformer_output_to_conv = nn.Sequential(nn.Linear(vit_dim, self.patch_dim), nn.LayerNorm(self.patch_dim))\n        d_ch = {'d0': 64, 'd1': 128, 'd2': 256, 'd3': 512, 'd4': 1024}\n        self.upsample = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n        self.X_0_0 = ConvBlock(64, d_ch['d0']); self.X_1_0 = ConvBlock(256, d_ch['d1'])\n        self.X_0_1 = ConvBlock(d_ch['d0'] + d_ch['d1'], d_ch['d0'])\n        self.X_2_0 = ConvBlock(512, d_ch['d2']); self.X_1_1 = ConvBlock(d_ch['d1'] + d_ch['d2'], d_ch['d1'])\n        self.X_0_2 = ConvBlock(d_ch['d0'] * 2 + d_ch['d1'], d_ch['d0'])\n        self.X_3_0 = ConvBlock(1024, d_ch['d3']); self.X_2_1 = ConvBlock(d_ch['d2'] + d_ch['d3'], d_ch['d2'])\n        self.X_1_2 = ConvBlock(d_ch['d1'] * 2 + d_ch['d2'], d_ch['d1'])\n        self.X_0_3 = ConvBlock(d_ch['d0'] * 3 + d_ch['d1'], d_ch['d0'])\n        self.X_4_0 = ConvBlock(2048, d_ch['d4']); self.X_3_1 = ConvBlock(d_ch['d3'] + d_ch['d4'], d_ch['d3'])\n        self.X_2_2 = ConvBlock(d_ch['d2'] * 2 + d_ch['d3'], d_ch['d2'])\n        self.X_1_3 = ConvBlock(d_ch['d1'] * 3 + d_ch['d2'], d_ch['d1'])\n        self.X_0_4 = ConvBlock(d_ch['d0'] * 4 + d_ch['d1'], d_ch['d0'])\n        self.final_conv = nn.Conv2d(d_ch['d0'], n_classes, kernel_size=1)\n    def forward(self, x):\n        e0 = self.encoder0(x); e1 = self.encoder1(e0); e2 = self.encoder2(e1); e3 = self.encoder3(e2); e4 = self.encoder4(e3)\n        bs, _, h, w = e4.shape\n        trans_in = self.patch_to_embedding(e4.flatten(2).transpose(1, 2)) + self.pos_embedding\n        trans_out = self.transformer_output_to_conv(self.transformer_encoder(trans_in)).transpose(1, 2).view(bs, self.patch_dim, h, w)\n        x0_0 = self.X_0_0(e0); x1_0 = self.X_1_0(e1); x0_1 = self.X_0_1(torch.cat([x0_0, self.upsample(x1_0)], 1))\n        x2_0 = self.X_2_0(e2); x1_1 = self.X_1_1(torch.cat([x1_0, self.upsample(x2_0)], 1)); x0_2 = self.X_0_2(torch.cat([x0_0, x0_1, self.upsample(x1_1)], 1))\n        x3_0 = self.X_3_0(e3); x2_1 = self.X_2_1(torch.cat([x2_0, self.upsample(x3_0)], 1)); x1_2 = self.X_1_2(torch.cat([x1_0, x1_1, self.upsample(x2_1)], 1))\n        x0_3 = self.X_0_3(torch.cat([x0_0, x0_1, x0_2, self.upsample(x1_2)], 1))\n        x4_0 = self.X_4_0(trans_out); x3_1 = self.X_3_1(torch.cat([x3_0, self.upsample(x4_0)], 1)); x2_2 = self.X_2_2(torch.cat([x2_0, x2_1, self.upsample(x3_1)], 1))\n        x1_3 = self.X_1_3(torch.cat([x1_0, x1_1, x1_2, self.upsample(x2_2)], 1)); x0_4 = self.X_0_4(torch.cat([x0_0, x0_1, x0_2, x0_3, self.upsample(x1_3)], 1))\n        return F.interpolate(self.final_conv(x0_4), scale_factor=2, mode='bilinear', align_corners=True)\n\nclass AttentionGate(nn.Module):\n    def __init__(self, F_g, F_l, F_int):\n        super().__init__(); self.W_g = nn.Sequential(nn.Conv2d(F_g, F_int, 1), nn.BatchNorm2d(F_int))\n        self.W_x = nn.Sequential(nn.Conv2d(F_l, F_int, 1), nn.BatchNorm2d(F_int))\n        self.psi = nn.Sequential(nn.Conv2d(F_int, 1, 1), nn.BatchNorm2d(1), nn.Sigmoid()); self.relu = nn.ReLU(inplace=True)\n    def forward(self, g, x): psi = self.relu(self.W_g(g) + self.W_x(x)); return x * self.psi(psi)\n\nclass AttentionUNet(nn.Module):\n    def __init__(self, n_classes=1):\n        super().__init__()\n        base = models.resnet50(weights=None); layers = list(base.children())\n        self.encoder0, self.encoder1 = nn.Sequential(*layers[:3]), nn.Sequential(*layers[3:5])\n        self.encoder2, self.encoder3, self.encoder4 = layers[5], layers[6], layers[7]\n        self.upconv3 = nn.ConvTranspose2d(2048, 1024, 2, 2); self.attn3 = AttentionGate(1024, 1024, 512); self.dec_conv3 = ConvBlock(2048, 1024)\n        self.upconv2 = nn.ConvTranspose2d(1024, 512, 2, 2); self.attn2 = AttentionGate(512, 512, 256); self.dec_conv2 = ConvBlock(1024, 512)\n        self.upconv1 = nn.ConvTranspose2d(512, 256, 2, 2); self.attn1 = AttentionGate(256, 256, 128); self.dec_conv1 = ConvBlock(512, 256)\n        self.upconv0 = nn.ConvTranspose2d(256, 64, 2, 2); self.attn0 = AttentionGate(64, 64, 32); self.dec_conv0 = ConvBlock(128, 64)\n        self.final_up = nn.ConvTranspose2d(64, 32, 2, 2); self.final_conv = nn.Conv2d(32, n_classes, 1)\n    def forward(self, x):\n        e0 = self.encoder0(x); e1 = self.encoder1(e0); e2 = self.encoder2(e1); e3 = self.encoder3(e2); e4 = self.encoder4(e3)\n        d3 = self.upconv3(e4); x3 = self.attn3(d3, e3); d3 = self.dec_conv3(torch.cat((x3, d3), 1))\n        d2 = self.upconv2(d3); x2 = self.attn2(d2, e2); d2 = self.dec_conv2(torch.cat((x2, d2), 1))\n        d1 = self.upconv1(d2); x1 = self.attn1(d1, e1); d1 = self.dec_conv1(torch.cat((x1, d1), 1))\n        d0 = self.upconv0(d1); x0 = self.attn0(d0, e0); d0 = self.dec_conv0(torch.cat((x0, d0), 1))\n        return self.final_conv(self.final_up(d0))\n\n# --- 6. LOADING SEGMENTATION MODELS ---\nmodels_loaded = False\nif test_df is not None and not test_df.empty:\n    try:\n        print(\"\\n--- Loading segmentation models... ---\")\n        model1 = UNetWithResNet50Encoder(n_classes=1).to(device)\n        model1.load_state_dict(torch.load(MODEL1_PATH, map_location=device))\n        model1.eval()\n        model2 = TransUNetPP(n_classes=1).to(device)\n        model2.load_state_dict(torch.load(MODEL2_PATH, map_location=device))\n        model2.eval()\n        model3 = AttentionUNet(n_classes=1).to(device)\n        model3.load_state_dict(torch.load(MODEL3_PATH, map_location=device))\n        model3.eval()\n        print(\"Segmentation models loaded successfully! âœ…\")\n        models_loaded = True\n    except Exception as e:\n        print(f\"Error loading segmentation model files (Check paths): {e}\")\n        models_loaded = False\nelse:\n    print(\"Skipping segmentation model loading as test data is unavailable.\")\n    \n# --- FOCAL LOSS IMPLEMENTATION ---\nclass FocalLoss(nn.Module):\n    def __init__(self, alpha=None, gamma=2., reduction='mean'):\n        super(FocalLoss, self).__init__()\n        self.gamma = gamma\n        self.alpha = alpha\n        self.reduction = reduction\n    def forward(self, input, target):\n        ce_loss = F.cross_entropy(input, target, reduction='none')\n        pt = torch.exp(-ce_loss)\n        loss = ((1 - pt) ** self.gamma * ce_loss)\n        if self.alpha is not None:\n            alpha_t = self.alpha[target.data.view(-1)]\n            loss = alpha_t * loss\n        if self.reduction == 'mean': return loss.mean()\n        else: return loss.sum()\n\n# =================================================================================\n# --- 7. CLASSIFICATION STAGE (HYBRID MODEL) ---\n# =================================================================================\n\nprint(\"\\n\\n--- Starting Classification Stage with Hybrid Model ---\")\nif not models_loaded or test_df.empty:\n    print(\"Skipping classification stage due to errors or missing data.\")\nelse:\n    try:\n        # --- 7.1 Generate Segmentation Predictions ---\n        print(\"\\nGenerating ensemble segmentation predictions...\")\n        train_seg_dataset = SegmentationInputDataset(train_df)\n        val_seg_dataset = SegmentationInputDataset(val_df)\n        test_seg_dataset = SegmentationInputDataset(test_df)\n\n        def get_segmentation_predictions(dataset, desc):\n            loader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=2)\n            all_preds = []\n            with torch.no_grad():\n                for images in tqdm(loader, desc=desc):\n                    images = images.to(device)\n                    # Ensemble predictions\n                    p1 = model1(images); p2 = model2(images); p3 = model3(images)\n                    ensembled = torch.sigmoid(torch.mean(torch.stack([p1, p2, p3]), dim=0))\n                    all_preds.append((ensembled > 0.5).float().cpu())\n            return torch.cat(all_preds)\n\n        train_pred_masks = get_segmentation_predictions(train_seg_dataset, \"Generating Train Masks\")\n        val_pred_masks = get_segmentation_predictions(val_seg_dataset, \"Generating Val Masks\")\n        test_pred_masks = get_segmentation_predictions(test_seg_dataset, \"Generating Test Masks\")\n        print(\"Segmentation predictions generated successfully.\")\n\n        # --- 7.2 Feature Engineering from Masks ---\n        print(\"\\nEngineering shape features from masks...\")\n\n        def get_shape_features(mask_tensor):\n            feature_df = pd.DataFrame()\n            for i in tqdm(range(len(mask_tensor)), desc=\"Calculating Features\"):\n                mask = mask_tensor[i, 0].numpy().astype(np.uint8)\n                # Ensure mask is 2D and binary\n                if mask.ndim == 3: mask = mask[:, :, 0] \n                mask = (mask > 0).astype(np.uint8)\n                \n                # Check for empty mask\n                if np.sum(mask) == 0:\n                    props_df = pd.DataFrame({'area': [0], 'perimeter': [0], 'solidity': [0], 'eccentricity': [0]})\n                else:\n                    try:\n                        # Use a small region minimum size to prevent issues with single pixels\n                        props = regionprops_table(mask, properties=('area', 'perimeter', 'solidity', 'eccentricity'))\n                        props_df = pd.DataFrame(props).head(1) # Take only the largest region if multiple exist\n                    except:\n                        props_df = pd.DataFrame({'area': [0], 'perimeter': [0], 'solidity': [0], 'eccentricity': [0]})\n                        \n                props_df['circularity'] = (4 * np.pi * props_df['area']) / (props_df['perimeter'] ** 2 + 1e-6)\n                feature_df = pd.concat([feature_df, props_df.head(1)], ignore_index=True)\n            feature_df.fillna(0, inplace=True)\n            return feature_df\n\n        train_features_df = get_shape_features(train_pred_masks)\n        val_features_df = get_shape_features(val_pred_masks)\n        test_features_df = get_shape_features(test_pred_masks)\n\n        # The shape features are: area, perimeter, solidity, eccentricity, circularity (5 features)\n        n_shape_features = train_features_df.shape[1] \n        scaler = StandardScaler()\n        train_features = scaler.fit_transform(train_features_df)\n        val_features = scaler.transform(val_features_df)\n        test_features = scaler.transform(test_features_df)\n        \n        # --- 7.3 Prepare Data and Datasets ---\n        train_transform = A.Compose([\n            A.Resize(height=IMAGE_SIZE[0], width=IMAGE_SIZE[1]), A.HorizontalFlip(p=0.5),\n            A.ShiftScaleRotate(shift_limit=0.05, scale_limit=0.05, rotate_limit=10, p=0.5),\n            A.RandomBrightnessContrast(p=0.3), A.GaussNoise(p=0.2), A.OpticalDistortion(p=0.2),\n            A.CoarseDropout(max_holes=8, max_height=8, max_width=8, p=0.2),\n            A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]), ToTensorV2()\n        ])\n        eval_transform = A.Compose([\n            A.Resize(height=IMAGE_SIZE[0], width=IMAGE_SIZE[1]), \n            A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]), ToTensorV2()\n        ])\n        minority_transform = A.Compose([ # More aggressive augmentation\n            A.Resize(height=IMAGE_SIZE[0], width=IMAGE_SIZE[1]), A.HorizontalFlip(p=0.8), A.VerticalFlip(p=0.5),\n            A.ShiftScaleRotate(shift_limit=0.1, scale_limit=0.1, rotate_limit=20, p=0.8),\n            A.RandomBrightnessContrast(brightness_limit=0.3, contrast_limit=0.3, p=0.5),\n            A.GaussNoise(p=0.3), A.OpticalDistortion(p=0.3),\n            A.CoarseDropout(max_holes=16, max_height=16, max_width=16, p=0.3),\n            A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]), ToTensorV2()\n        ])\n\n        class HybridDataset(Dataset):\n            def __init__(self, dataframe, pred_masks, shape_features, transform, minority_classes=None, minority_transform=None):\n                self.df = dataframe.reset_index(drop=True)\n                self.masks = pred_masks\n                self.features = shape_features\n                self.transform = transform\n                self.minority_classes = minority_classes if minority_classes else []\n                self.minority_transform = minority_transform if minority_transform else transform\n\n            def __len__(self): return len(self.df)\n\n            def __getitem__(self, idx):\n                row = self.df.iloc[idx]\n                image = cv2.imread(row['image_file_path'])\n                image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n                label = row['class_label']\n                \n                # Conditional augmentation\n                if label in self.minority_classes and self.minority_transform:\n                    transformed = self.minority_transform(image=image)\n                else:\n                    transformed = self.transform(image=image)\n\n                img_tensor = transformed['image']\n                \n                # Apply mask to the image (Liver only)\n                # Ensure mask is the right size/shape for element-wise multiplication\n                pred_mask = self.masks[idx].to(img_tensor.device) \n                masked_image = img_tensor * pred_mask\n                \n                shape_feats = torch.tensor(self.features[idx], dtype=torch.float)\n                label = torch.tensor(label, dtype=torch.long)\n                return masked_image, shape_feats, label\n\n        # Use setting from previous best run: only 'Severe' (2) gets strong augmentation\n        minority_classes_list = [2] \n        train_cls_dataset = HybridDataset(\n            train_df, train_pred_masks, train_features, \n            transform=train_transform, minority_classes=minority_classes_list, minority_transform=minority_transform\n        )\n        val_cls_dataset = HybridDataset(val_df, val_pred_masks, val_features, eval_transform)\n        test_cls_dataset = HybridDataset(test_df, test_pred_masks, test_features, eval_transform)\n        \n        # WeightedRandomSampler for class balancing\n        class_counts = train_df['class_label'].value_counts().to_dict()\n        num_samples = len(train_df)\n        class_weights_sampler = {i: num_samples / class_counts.get(i, 1) for i in range(NUM_CLASSES)}\n        sample_weights = np.array([class_weights_sampler[t] for t in train_df['class_label'].values])\n        sampler = torch.utils.data.WeightedRandomSampler(\n            weights=torch.from_numpy(sample_weights).double(), num_samples=len(sample_weights), replacement=True\n        )\n\n        train_cls_loader = DataLoader(train_cls_dataset, batch_size=BATCH_SIZE, sampler=sampler, num_workers=2, pin_memory=True)\n        val_cls_loader = DataLoader(val_cls_dataset, BATCH_SIZE, shuffle=False, num_workers=2, pin_memory=True)\n        test_cls_loader = DataLoader(test_cls_dataset, BATCH_SIZE, shuffle=False, num_workers=2, pin_memory=True)\n        print(\"\\nHybrid datasets and loaders are ready with WeightedRandomSampler. ğŸ“Š\")\n\n        # --- 7.4 HYBRID MODEL ARCHITECTURE (Dynamic Encoder) ---\n        \n        # PLACEHOLDER MODELS (since external weights/complex definitions are unavailable)\n        class CoAnNetFeatureExtractor(nn.Module):\n            # CoAnNet is complex; we use a simpler block with the expected output dimension\n            def __init__(self):\n                super().__init__()\n                self.conv = nn.Conv2d(3, 512, kernel_size=3, padding=1)\n                self.pool = nn.AdaptiveAvgPool2d((1, 1))\n                self.identity = nn.Identity()\n            def forward(self, x):\n                x = F.relu(self.conv(x))\n                return self.identity(self.pool(x).flatten(1))\n\n        class SimCLRResNet50(nn.Module):\n            # SimCLR is ResNet50 with special weights; we use a standard pre-trained one\n            def __init__(self):\n                super().__init__()\n                self.base = models.resnet50(weights=models.ResNet50_Weights.IMAGENET1K_V1)\n                self.base.fc = nn.Identity()\n            def forward(self, x):\n                return self.base(x)\n\n\n        class HybridClassifier(nn.Module):\n            def __init__(self, cnn_name: str, n_classes=3, n_shape_features=5):\n                super().__init__()\n                \n                cnn_feature_dim = 0\n                \n                if cnn_name == 'resnet34':\n                    self.cnn_base = models.resnet34(weights=models.ResNet34_Weights.DEFAULT)\n                    cnn_feature_dim = self.cnn_base.fc.in_features\n                    self.cnn_base.fc = nn.Identity()\n                elif cnn_name == 'resnet50' or cnn_name == 'ResNet50_GAP':\n                    # ResNet50 and ResNet50_GAP are the same in feature extraction layer, GAP is handled by AdaptiveAvgPool2d.\n                    self.cnn_base = models.resnet50(weights=models.ResNet50_Weights.DEFAULT)\n                    cnn_feature_dim = self.cnn_base.fc.in_features\n                    self.cnn_base.fc = nn.Identity()\n                elif cnn_name == 'densenet121':\n                    self.cnn_base = models.densenet121(weights=models.DenseNet121_Weights.DEFAULT)\n                    cnn_feature_dim = self.cnn_base.classifier.in_features\n                    self.cnn_base.classifier = nn.Identity()\n                elif cnn_name == 'EfficientNet_B0':\n                    self.cnn_base = models.efficientnet_b0(weights=models.EfficientNet_B0_Weights.DEFAULT)\n                    cnn_feature_dim = self.cnn_base.classifier[1].in_features\n                    self.cnn_base.classifier = nn.Identity() # Remove the default classifier\n                elif cnn_name == 'SimCLR_ResNet50':\n                    self.cnn_base = SimCLRResNet50() # Custom/Pre-loaded weights are required for true SimCLR\n                    cnn_feature_dim = 2048\n                elif cnn_name == 'CoAnNet':\n                    self.cnn_base = CoAnNetFeatureExtractor() # Placeholder\n                    cnn_feature_dim = 512 # Placeholder dimension\n                else:\n                    raise ValueError(f\"Unknown CNN name: {cnn_name}\")\n                    \n                print(f\"Loaded {cnn_name} encoder (Dim: {cnn_feature_dim})\")\n\n                self.tabular_mlp = nn.Sequential(\n                    nn.Linear(n_shape_features, 64), nn.ReLU(), nn.Dropout(0.4),\n                    nn.Linear(64, 32), nn.ReLU(), nn.Dropout(0.4)\n                )\n                self.fusion_classifier = nn.Sequential(\n                    nn.Linear(cnn_feature_dim + 32, 256), nn.ReLU(), nn.Dropout(0.5),\n                    nn.Linear(256, n_classes)\n                )\n                \n            def forward(self, image, shape_features):\n                cnn_features = self.cnn_base(image)\n                # Ensure 1D output after CNN\n                if cnn_features.ndim > 2:\n                    # Adaptive Avg Pool for any remaining spatial dimensions (e.g., in ResNet/EfficientNet backbone output)\n                    cnn_features = F.adaptive_avg_pool2d(cnn_features, (1, 1)).flatten(1) \n                    \n                tabular_features = self.tabular_mlp(shape_features)\n                combined_features = torch.cat([cnn_features, tabular_features], dim=1)\n                output = self.fusion_classifier(combined_features)\n                return output\n\n        # --- 7.5 TRAIN AND EVALUATE FUNCTION PER MODEL ---\n        def train_and_evaluate_single_model(cnn_name: str, train_loader, val_loader, test_loader, class_weights, n_shape_features, device, num_epochs=NUM_EPOCHS, patience=10):\n            \"\"\"Trains and evaluates a single hybrid model.\"\"\"\n            print(f\"\\n=============================================\")\n            print(f\"ğŸš€ Starting Training for Model: {cnn_name}\")\n            print(f\"=============================================\")\n            \n            cls_model = HybridClassifier(cnn_name=cnn_name, n_classes=NUM_CLASSES, n_shape_features=n_shape_features).to(device)\n            \n            # Use gamma=2.0 as it gave the best result so far\n            criterion = FocalLoss(alpha=class_weights, gamma=2.0)\n            \n            optimizer = torch.optim.AdamW(cls_model.parameters(), lr=1e-4, weight_decay=1e-5)\n            scheduler = ReduceLROnPlateau(optimizer, mode='max', factor=0.2, patience=5, verbose=False)\n            \n            epochs_no_improve = 0; best_val_accuracy = 0.0\n            model_save_path = f'best_classifier_model_{cnn_name}.pth'\n            \n            for epoch in range(num_epochs):\n                cls_model.train()\n                running_loss = 0.0\n                for images, shapes, labels in train_loader:\n                    images, shapes, labels = images.to(device), shapes.to(device), labels.to(device)\n                    optimizer.zero_grad()\n                    outputs = cls_model(images, shapes)\n                    loss = criterion(outputs, labels); loss.backward(); optimizer.step()\n                    running_loss += loss.item()\n                \n                cls_model.eval()\n                val_corrects = 0\n                with torch.no_grad():\n                    for images, shapes, labels in val_loader:\n                        images, shapes, labels = images.to(device), shapes.to(device), labels.to(device)\n                        outputs = cls_model(images, shapes); _, preds = torch.max(outputs, 1)\n                        val_corrects += torch.sum(preds == labels.data)\n\n                val_accuracy = val_corrects.double() / len(val_loader.dataset)\n                scheduler.step(val_accuracy)\n\n                # Early stopping logic\n                if val_accuracy > best_val_accuracy:\n                    best_val_accuracy = val_accuracy\n                    torch.save(cls_model.state_dict(), model_save_path)\n                    epochs_no_improve = 0\n                else:\n                    epochs_no_improve += 1\n                    \n                if epochs_no_improve >= patience:\n                    print(f\"â¹ï¸ Early stopping triggered for {cnn_name} at epoch {epoch+1}.\")\n                    break\n                    \n            print(f\"Final Best Validation Accuracy for {cnn_name}: {best_val_accuracy:.4f}\")\n\n            # Final Test Evaluation\n            try:\n                cls_model.load_state_dict(torch.load(model_save_path, map_location=device))\n            except Exception as e:\n                 print(f\"âš ï¸ Could not load best weights for {cnn_name}. Using current weights for testing. Error: {e}\")\n            cls_model.eval()\n            all_labels, all_preds = [], []\n            with torch.no_grad():\n                for images, shapes, labels in test_loader:\n                    images, shapes, labels = images.to(device), shapes.to(device), labels.to(device)\n                    outputs = cls_model(images, shapes); _, preds = torch.max(outputs, 1)\n                    all_labels.extend(labels.cpu().numpy()); all_preds.extend(preds.cpu().numpy())\n            \n            report = classification_report(all_labels, all_preds, target_names=CLASS_NAMES, zero_division=0, output_dict=True)\n            return np.array(all_preds), report\n\n        # --- 7.6 MASTER EXECUTION AND COMPARISON ---\n        \n        # ADD ALL MODELS TO BE EVALUATED\n        model_list = [\n            'resnet34', \n            'resnet50', \n            'densenet121', \n            'EfficientNet_B0',\n            'ResNet50_GAP',      # Standard ResNet50 encoder using GAP\n            'SimCLR_ResNet50',   # ResNet50 with placeholder SimCLR weights\n            'CoAnNet'            # Placeholder architecture\n        ]\n        results = {} \n\n        for model_name in model_list:\n            preds, report = train_and_evaluate_single_model(\n                cnn_name=model_name,\n                train_loader=train_cls_loader, val_loader=val_cls_loader, test_loader=test_cls_loader,\n                class_weights=class_weights, n_shape_features=n_shape_features, device=device\n            )\n            results[model_name] = {'preds': preds, 'report': report}\n\n        # --- 7.7 FINAL ENSEMBLE SELECTION AND REPORT ---\n        print(\"\\n=============================================\")\n        print(\"ğŸ† FINAL RESULT SELECTION (PER-CLASS BEST RECALL)\")\n        print(\"=============================================\")\n\n        # 1. Get ground truth labels\n        all_labels = np.concatenate([labels.cpu().numpy() for _, _, labels in test_cls_loader])\n        final_preds = np.zeros_like(all_labels)\n        \n        best_model_per_class = {}\n        \n        # 2. Determine the best model for each class based on 'recall' (accuracy per class)\n        comparison_data = []\n        for i, class_name in enumerate(CLASS_NAMES):\n            best_recall = -1.0\n            best_model = None\n            \n            row = {'Class': class_name}\n            for model_name, res in results.items():\n                recall = res['report'].get(class_name, {}).get('recall', 0.0) # Handle missing class gracefully\n                row[model_name] = f'{recall:.4f}'\n                \n                if recall > best_recall:\n                    best_recall = recall\n                    best_model = model_name\n                    \n            best_model_per_class[i] = best_model\n            row['Best Model'] = best_model\n            row['Best Recall'] = f'{best_recall:.4f}'\n            comparison_data.append(row)\n            \n            # 3. Create the final combined prediction array (Per-Class Best Model Selection)\n            class_indices = np.where(all_labels == i)[0]\n            if best_model:\n                 final_preds[class_indices] = results[best_model]['preds'][class_indices]\n\n        # Print comparison table\n        comparison_df = pd.DataFrame(comparison_data)\n        print(\"\\n--- PER-CLASS RECALL COMPARISON ---\")\n        print(comparison_df.to_markdown(index=False))\n        \n        # Print Final Combined Report\n        print(\"\\n--- FINAL CLASSIFICATION REPORT (Combined Best Recall) ---\")\n        print(classification_report(all_labels, final_preds, target_names=CLASS_NAMES, zero_division=0))\n\n        # Confusion Matrix for the Combined Result\n        cm_final = confusion_matrix(all_labels, final_preds, labels=range(NUM_CLASSES))\n        disp_final = ConfusionMatrixDisplay(confusion_matrix=cm_final, display_labels=CLASS_NAMES)\n        fig, ax = plt.subplots(figsize=(8, 8))\n        disp_final.plot(ax=ax, cmap=plt.cm.Greens)\n        plt.title(\"Confusion Matrix (Best Per-Class Selection)\")\n        plt.show()\n\n    except Exception as e:\n        print(f\"\\nAn error occurred in the Classification Pipeline: {e}\")\n        import traceback\n        traceback.print_exc()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-11T15:35:53.033444Z","iopub.execute_input":"2025-11-11T15:35:53.033752Z","iopub.status.idle":"2025-11-11T17:51:37.111713Z","shell.execute_reply.started":"2025-11-11T15:35:53.033727Z","shell.execute_reply":"2025-11-11T17:51:37.110729Z"}},"outputs":[{"name":"stdout","text":"Using device: cuda âš™ï¸\n\n--- Creating DataFrames ---\nSuccessfully created dataframes: 5364 train, 674 val, 664 test.\nOriginal calculated weights: [0.82055989 0.8641856  1.60215054]\nBoosted 'Moderate' weight. New weights: [0.82055989 1.2962784  1.60215054]\n\nFinal Class Weights for Focal Loss: tensor([0.8206, 1.2963, 1.6022], device='cuda:0')\n\n--- Loading segmentation models... ---\nSegmentation models loaded successfully! âœ…\n\n\n--- Starting Classification Stage with Hybrid Model ---\n\nGenerating ensemble segmentation predictions...\n","output_type":"stream"},{"name":"stderr","text":"Generating Train Masks: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 336/336 [03:09<00:00,  1.77it/s]\nGenerating Val Masks: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 43/43 [00:26<00:00,  1.63it/s]\nGenerating Test Masks: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 42/42 [00:26<00:00,  1.59it/s]\n","output_type":"stream"},{"name":"stdout","text":"Segmentation predictions generated successfully.\n\nEngineering shape features from masks...\n","output_type":"stream"},{"name":"stderr","text":"Calculating Features: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5364/5364 [00:23<00:00, 231.14it/s]\nCalculating Features: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 674/674 [00:02<00:00, 231.71it/s]\nCalculating Features: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 664/664 [00:02<00:00, 231.08it/s]\n","output_type":"stream"},{"name":"stdout","text":"\nHybrid datasets and loaders are ready with WeightedRandomSampler. ğŸ“Š\n\n=============================================\nğŸš€ Starting Training for Model: resnet34\n=============================================\n","output_type":"stream"},{"name":"stderr","text":"Downloading: \"https://download.pytorch.org/models/resnet34-b627a593.pth\" to /root/.cache/torch/hub/checkpoints/resnet34-b627a593.pth\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 83.3M/83.3M [00:00<00:00, 188MB/s] \n","output_type":"stream"},{"name":"stdout","text":"Loaded resnet34 encoder (Dim: 512)\nâ¹ï¸ Early stopping triggered for resnet34 at epoch 13.\nFinal Best Validation Accuracy for resnet34: 0.7315\n\n=============================================\nğŸš€ Starting Training for Model: resnet50\n=============================================\n","output_type":"stream"},{"name":"stderr","text":"Downloading: \"https://download.pytorch.org/models/resnet50-11ad3fa6.pth\" to /root/.cache/torch/hub/checkpoints/resnet50-11ad3fa6.pth\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 97.8M/97.8M [00:00<00:00, 207MB/s] \n","output_type":"stream"},{"name":"stdout","text":"Loaded resnet50 encoder (Dim: 2048)\nâ¹ï¸ Early stopping triggered for resnet50 at epoch 17.\nFinal Best Validation Accuracy for resnet50: 0.6869\n\n=============================================\nğŸš€ Starting Training for Model: densenet121\n=============================================\n","output_type":"stream"},{"name":"stderr","text":"Downloading: \"https://download.pytorch.org/models/densenet121-a639ec97.pth\" to /root/.cache/torch/hub/checkpoints/densenet121-a639ec97.pth\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 30.8M/30.8M [00:00<00:00, 153MB/s] ","output_type":"stream"},{"name":"stdout","text":"Loaded densenet121 encoder (Dim: 1024)\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"â¹ï¸ Early stopping triggered for densenet121 at epoch 22.\nFinal Best Validation Accuracy for densenet121: 0.6884\n","output_type":"stream"},{"name":"stderr","text":"Downloading: \"https://download.pytorch.org/models/efficientnet_b0_rwightman-7f5810bc.pth\" to /root/.cache/torch/hub/checkpoints/efficientnet_b0_rwightman-7f5810bc.pth\n","output_type":"stream"},{"name":"stdout","text":"\n=============================================\nğŸš€ Starting Training for Model: EfficientNet_B0\n=============================================\n","output_type":"stream"},{"name":"stderr","text":"100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20.5M/20.5M [00:00<00:00, 135MB/s] \n","output_type":"stream"},{"name":"stdout","text":"Loaded EfficientNet_B0 encoder (Dim: 1280)\nâ¹ï¸ Early stopping triggered for EfficientNet_B0 at epoch 19.\nFinal Best Validation Accuracy for EfficientNet_B0: 0.6499\n\n=============================================\nğŸš€ Starting Training for Model: ResNet50_GAP\n=============================================\nLoaded ResNet50_GAP encoder (Dim: 2048)\nâ¹ï¸ Early stopping triggered for ResNet50_GAP at epoch 24.\nFinal Best Validation Accuracy for ResNet50_GAP: 0.7047\n\n=============================================\nğŸš€ Starting Training for Model: SimCLR_ResNet50\n=============================================\n","output_type":"stream"},{"name":"stderr","text":"Downloading: \"https://download.pytorch.org/models/resnet50-0676ba61.pth\" to /root/.cache/torch/hub/checkpoints/resnet50-0676ba61.pth\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 97.8M/97.8M [00:00<00:00, 189MB/s] \n","output_type":"stream"},{"name":"stdout","text":"Loaded SimCLR_ResNet50 encoder (Dim: 2048)\nâ¹ï¸ Early stopping triggered for SimCLR_ResNet50 at epoch 18.\nFinal Best Validation Accuracy for SimCLR_ResNet50: 0.6766\n\n=============================================\nğŸš€ Starting Training for Model: CoAnNet\n=============================================\nLoaded CoAnNet encoder (Dim: 512)\nâ¹ï¸ Early stopping triggered for CoAnNet at epoch 41.\nFinal Best Validation Accuracy for CoAnNet: 0.3353\n\n=============================================\nğŸ† FINAL RESULT SELECTION (PER-CLASS BEST RECALL)\n=============================================\n\n--- PER-CLASS RECALL COMPARISON ---\n| Class    |   resnet34 |   resnet50 |   densenet121 |   EfficientNet_B0 |   ResNet50_GAP |   SimCLR_ResNet50 |   CoAnNet | Best Model      |   Best Recall |\n|:---------|-----------:|-----------:|--------------:|------------------:|---------------:|------------------:|----------:|:----------------|--------------:|\n| Mild     |     0.6429 |     0.56   |        0.6543 |            0.7314 |         0.58   |            0.7057 |    0      | EfficientNet_B0 |        0.7314 |\n| Moderate |     0.3158 |     0.6992 |        0.609  |            0.7293 |         0.3609 |            0.4962 |    0.8045 | CoAnNet         |        0.8045 |\n| Severe   |     0.6354 |     0.3536 |        0.3536 |            0.1989 |         0.4917 |            0.326  |    0.232  | resnet34        |        0.6354 |\n\n--- FINAL CLASSIFICATION REPORT (Combined Best Recall) ---\n              precision    recall  f1-score   support\n\n        Mild       0.94      0.73      0.82       350\n    Moderate       0.44      0.80      0.57       133\n      Severe       0.78      0.64      0.70       181\n\n    accuracy                           0.72       664\n   macro avg       0.72      0.72      0.70       664\nweighted avg       0.80      0.72      0.74       664\n\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<Figure size 800x800 with 2 Axes>","image/png":"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\n"},"metadata":{}}],"execution_count":2},{"cell_type":"code","source":"import os\nimport torch\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom PIL import Image\nimport torch.nn.functional as F\nimport torch.nn as nn\nfrom torchvision import models, transforms\nfrom torch.utils.data import DataLoader, Dataset\nfrom tqdm import tqdm\nfrom sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay\nfrom sklearn.utils.class_weight import compute_class_weight\nfrom torch.optim.lr_scheduler import ReduceLROnPlateau\nimport albumentations as A\nfrom albumentations.pytorch import ToTensorV2\nimport cv2\nfrom skimage.measure import regionprops_table\nfrom sklearn.preprocessing import StandardScaler\nimport warnings\n\n# Suppress minor warnings for clean output\nwarnings.filterwarnings(\"ignore\", category=UserWarning)\n\n# --- 1. CONFIGURATION ---\n# NOTE: Replace these paths with actual working paths if running outside a Kaggle environment\nMODEL1_PATH = '/kaggle/input/ourensemble5/3URESNET/best_segmentation_model1.pth' # UNet with ResNet50\nMODEL2_PATH = '/kaggle/input/ourensemble5/3URESNET/best_transunetpp_model.pth' # TransUNetPP\nMODEL3_PATH = '/kaggle/input/ourensemble5/3URESNET/best_segmentation_model3.pth' # AttentionUNet\nMETADATA_PATH = '/kaggle/input/t2metadataaa/T2_age_gender_evaluation.csv'\nTEST_DIR = '/kaggle/input/dataa1/Cirrhosis_T2_2D/test'\nTRAIN_DIR = '/kaggle/input/dataa1/Cirrhosis_T2_2D/train'\nVAL_DIR = '/kaggle/input/dataa1/Cirrhosis_T2_2D/valid'\nIMAGE_SIZE = (224, 224)\nCLASS_NAMES = ['Mild', 'Moderate', 'Severe']\nNUM_CLASSES = len(CLASS_NAMES)\nNUM_EPOCHS = 50\nBATCH_SIZE = 16\n\n# --- 2. SETUP ---\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device} âš™ï¸\")\n\n# --- 3. DATAFRAME CREATION AND MERGE ---\nprint(\"\\n--- Creating DataFrames ---\")\ntry:\n    # Load and clean metadata\n    metadata_df = pd.read_csv(METADATA_PATH)\n    metadata_df.rename(columns={'Patient ID': 'ID', 'Radiological Evaluation': 'radiological_evaluation'}, inplace=True)\n    metadata_df.dropna(subset=['radiological_evaluation'], inplace=True)\n    metadata_df['radiological_evaluation'] = metadata_df['radiological_evaluation'].astype(int)\n    metadata_df['ID'] = metadata_df['ID'].astype(str)\n    class_label_map = {1: 0, 2: 1, 3: 2} # Mild=0, Moderate=1, Severe=2\n    metadata_df['class_label'] = metadata_df['radiological_evaluation'].map(class_label_map)\n    metadata_df.dropna(subset=['class_label'], inplace=True)\n    valid_ids = set(metadata_df['ID'].tolist())\n\n    # Helper function to create dataframe from file structure\n    def create_dataframe_from_ids(directory, allowed_ids):\n        data = []\n        if not os.path.exists(directory): return pd.DataFrame(data)\n        for folder_name in os.listdir(directory):\n            if folder_name in allowed_ids:\n                folder_path = os.path.join(directory, folder_name)\n                images_dir = os.path.join(folder_path, 'images')\n                if os.path.exists(images_dir):\n                    for image_file in os.listdir(images_dir):\n                        mask_path = os.path.join(folder_path, 'masks', image_file)\n                        if os.path.exists(mask_path):\n                            image_path = os.path.join(images_dir, image_file)\n                            data.append((folder_name, image_path, mask_path))\n        return pd.DataFrame(data, columns=['ID', 'image_file_path', 'mask_file_path'])\n\n    # Merge dataframes\n    train_df = pd.merge(create_dataframe_from_ids(TRAIN_DIR, valid_ids), metadata_df, on='ID')\n    val_df = pd.merge(create_dataframe_from_ids(VAL_DIR, valid_ids), metadata_df, on='ID')\n    test_df = pd.merge(create_dataframe_from_ids(TEST_DIR, valid_ids), metadata_df, on='ID')\n\n    if train_df.empty or val_df.empty or test_df.empty:\n        raise ValueError(\"One or more dataframes are empty. Check your file paths and metadata.\")\n    \n    print(f\"Successfully created dataframes: {len(train_df)} train, {len(val_df)} val, {len(test_df)} test.\")\n    \n    # *** UPDATED HERE ***\n    # Calculate class weights for Focal Loss\n    class_weights_array = compute_class_weight('balanced', classes=np.unique(train_df['class_label'].values), y=train_df['class_label'].values)\n    print(f\"Original calculated weights: {class_weights_array}\")\n    \n    # --- NEW: Manually boost the weight for the 'Moderate' class (label 1) ---\n    moderate_boost_factor = 1.2  # <-- UPDATED from 1.5 to 1.2 (less aggressive boost)\n    class_weights_array[1] = class_weights_array[1] * moderate_boost_factor\n    \n    class_weights = torch.tensor(class_weights_array, dtype=torch.float).to(device)\n    print(f\"Boosted 'Moderate' weight. New weights: {class_weights_array}\")\n    print(f\"\\nFinal Class Weights for Focal Loss: {class_weights}\")\n\n\nexcept Exception as e:\n    print(f\"Error during data setup: {e}\")\n    test_df = None\n\n# --- 4. DATASET FOR INITIAL SEGMENTATION ---\nclass SegmentationInputDataset(Dataset):\n    def __init__(self, dataframe):\n        self.dataframe = dataframe\n        # Standard normalization for CNNs (e.g., ImageNet weights)\n        self.transform = transforms.Compose([\n            transforms.Resize(IMAGE_SIZE, transforms.InterpolationMode.BILINEAR),\n            transforms.ToTensor(),\n            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n        ])\n    def __len__(self): return len(self.dataframe)\n    def __getitem__(self, idx):\n        row = self.dataframe.iloc[idx]\n        image_pil = Image.open(row['image_file_path']).convert('RGB')\n        return self.transform(image_pil)\n\n# --- 5. MODEL ARCHITECTURE DEFINITIONS (Segmentation Models) ---\n# NOTE: Using the user's provided segmentation model definitions\n\nclass ConvBlock(nn.Module):\n    def __init__(self, in_ch, out_ch):\n        super().__init__()\n        self.conv = nn.Sequential(\n            nn.Conv2d(in_ch, out_ch, 3, 1, 1, bias=False), nn.BatchNorm2d(out_ch), nn.ReLU(inplace=True),\n            nn.Conv2d(out_ch, out_ch, 3, 1, 1, bias=False), nn.BatchNorm2d(out_ch), nn.ReLU(inplace=True))\n    def forward(self, x): return self.conv(x)\n\nclass DecoderBlock(nn.Module):\n    def __init__(self, in_channels, skip_channels, out_channels):\n        super().__init__()\n        self.conv1 = nn.Conv2d(in_channels + skip_channels, out_channels, kernel_size=3, padding=1, bias=False)\n        self.bn1 = nn.BatchNorm2d(out_channels)\n        self.relu = nn.ReLU(inplace=True)\n        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1, bias=False)\n        self.bn2 = nn.BatchNorm2d(out_channels)\n    def forward(self, x, skip_connection):\n        x = F.interpolate(x, scale_factor=2, mode='bilinear', align_corners=True)\n        x = torch.cat([x, skip_connection], dim=1)\n        x = self.relu(self.bn1(self.conv1(x)))\n        x = self.relu(self.bn2(self.conv2(x)))\n        return x\n\nclass UNetWithResNet50Encoder(nn.Module):\n    def __init__(self, n_classes=1):\n        super().__init__()\n        base_model = models.resnet50(weights=None)\n        base_layers = list(base_model.children())\n        self.encoder0, self.encoder1 = nn.Sequential(*base_layers[:3]), nn.Sequential(*base_layers[3:5])\n        self.encoder2, self.encoder3, self.encoder4 = base_layers[5], base_layers[6], base_layers[7]\n        self.decoder3 = DecoderBlock(2048, 1024, 512)\n        self.decoder2 = DecoderBlock(512, 512, 256)\n        self.decoder1 = DecoderBlock(256, 256, 128)\n        self.decoder0 = DecoderBlock(128, 64, 64)\n        self.final_conv = nn.Conv2d(64, n_classes, kernel_size=1)\n    def forward(self, x):\n        e0 = self.encoder0(x); e1 = self.encoder1(e0); e2 = self.encoder2(e1); e3 = self.encoder3(e2); e4 = self.encoder4(e3)\n        d3 = self.decoder3(e4, e3); d2 = self.decoder2(d3, e2); d1 = self.decoder1(d2, e1); d0 = self.decoder0(d1, e0)\n        out = F.interpolate(d0, scale_factor=2, mode='bilinear', align_corners=True)\n        return self.final_conv(out)\n\nclass TransUNetPP(nn.Module):\n    def __init__(self, n_classes=1, img_dim=224, vit_dim=768, vit_depth=12, vit_heads=12):\n        super().__init__()\n        base_model = models.resnet50(weights=None)\n        base_layers = list(base_model.children())\n        self.encoder0, self.encoder1 = nn.Sequential(*base_layers[:3]), nn.Sequential(*base_layers[3:5])\n        self.encoder2, self.encoder3, self.encoder4 = base_layers[5], base_layers[6], base_layers[7]\n        num_patches, self.patch_dim = (img_dim // 32) ** 2, 2048\n        self.pos_embedding = nn.Parameter(torch.randn(1, num_patches, vit_dim))\n        self.patch_to_embedding = nn.Linear(self.patch_dim, vit_dim)\n        transformer_layer = nn.TransformerEncoderLayer(d_model=vit_dim, nhead=vit_heads, dim_feedforward=vit_dim * 4, batch_first=True)\n        self.transformer_encoder = nn.TransformerEncoder(transformer_layer, num_layers=vit_depth)\n        self.transformer_output_to_conv = nn.Sequential(nn.Linear(vit_dim, self.patch_dim), nn.LayerNorm(self.patch_dim))\n        d_ch = {'d0': 64, 'd1': 128, 'd2': 256, 'd3': 512, 'd4': 1024}\n        self.upsample = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n        self.X_0_0 = ConvBlock(64, d_ch['d0']); self.X_1_0 = ConvBlock(256, d_ch['d1'])\n        self.X_0_1 = ConvBlock(d_ch['d0'] + d_ch['d1'], d_ch['d0'])\n        self.X_2_0 = ConvBlock(512, d_ch['d2']); self.X_1_1 = ConvBlock(d_ch['d1'] + d_ch['d2'], d_ch['d1'])\n        self.X_0_2 = ConvBlock(d_ch['d0'] * 2 + d_ch['d1'], d_ch['d0'])\n        self.X_3_0 = ConvBlock(1024, d_ch['d3']); self.X_2_1 = ConvBlock(d_ch['d2'] + d_ch['d3'], d_ch['d2'])\n        self.X_1_2 = ConvBlock(d_ch['d1'] * 2 + d_ch['d2'], d_ch['d1'])\n        self.X_0_3 = ConvBlock(d_ch['d0'] * 3 + d_ch['d1'], d_ch['d0'])\n        self.X_4_0 = ConvBlock(2048, d_ch['d4']); self.X_3_1 = ConvBlock(d_ch['d3'] + d_ch['d4'], d_ch['d3'])\n        self.X_2_2 = ConvBlock(d_ch['d2'] * 2 + d_ch['d3'], d_ch['d2'])\n        self.X_1_3 = ConvBlock(d_ch['d1'] * 3 + d_ch['d2'], d_ch['d1'])\n        self.X_0_4 = ConvBlock(d_ch['d0'] * 4 + d_ch['d1'], d_ch['d0'])\n        self.final_conv = nn.Conv2d(d_ch['d0'], n_classes, kernel_size=1)\n    def forward(self, x):\n        e0 = self.encoder0(x); e1 = self.encoder1(e0); e2 = self.encoder2(e1); e3 = self.encoder3(e2); e4 = self.encoder4(e3)\n        bs, _, h, w = e4.shape\n        trans_in = self.patch_to_embedding(e4.flatten(2).transpose(1, 2)) + self.pos_embedding\n        trans_out = self.transformer_output_to_conv(self.transformer_encoder(trans_in)).transpose(1, 2).view(bs, self.patch_dim, h, w)\n        x0_0 = self.X_0_0(e0); x1_0 = self.X_1_0(e1); x0_1 = self.X_0_1(torch.cat([x0_0, self.upsample(x1_0)], 1))\n        x2_0 = self.X_2_0(e2); x1_1 = self.X_1_1(torch.cat([x1_0, self.upsample(x2_0)], 1)); x0_2 = self.X_0_2(torch.cat([x0_0, x0_1, self.upsample(x1_1)], 1))\n        x3_0 = self.X_3_0(e3); x2_1 = self.X_2_1(torch.cat([x2_0, self.upsample(x3_0)], 1)); x1_2 = self.X_1_2(torch.cat([x1_0, x1_1, self.upsample(x2_1)], 1))\n        x0_3 = self.X_0_3(torch.cat([x0_0, x0_1, x0_2, self.upsample(x1_2)], 1))\n        x4_0 = self.X_4_0(trans_out); x3_1 = self.X_3_1(torch.cat([x3_0, self.upsample(x4_0)], 1)); x2_2 = self.X_2_2(torch.cat([x2_0, x2_1, self.upsample(x3_1)], 1))\n        x1_3 = self.X_1_3(torch.cat([x1_0, x1_1, x1_2, self.upsample(x2_2)], 1)); x0_4 = self.X_0_4(torch.cat([x0_0, x0_1, x0_2, x0_3, self.upsample(x1_3)], 1))\n        return F.interpolate(self.final_conv(x0_4), scale_factor=2, mode='bilinear', align_corners=True)\n\nclass AttentionGate(nn.Module):\n    def __init__(self, F_g, F_l, F_int):\n        super().__init__(); self.W_g = nn.Sequential(nn.Conv2d(F_g, F_int, 1), nn.BatchNorm2d(F_int))\n        self.W_x = nn.Sequential(nn.Conv2d(F_l, F_int, 1), nn.BatchNorm2d(F_int))\n        self.psi = nn.Sequential(nn.Conv2d(F_int, 1, 1), nn.BatchNorm2d(1), nn.Sigmoid()); self.relu = nn.ReLU(inplace=True)\n    def forward(self, g, x): psi = self.relu(self.W_g(g) + self.W_x(x)); return x * self.psi(psi)\n\nclass AttentionUNet(nn.Module):\n    def __init__(self, n_classes=1):\n        super().__init__()\n        base = models.resnet50(weights=None); layers = list(base.children())\n        self.encoder0, self.encoder1 = nn.Sequential(*layers[:3]), nn.Sequential(*layers[3:5])\n        self.encoder2, self.encoder3, self.encoder4 = layers[5], layers[6], layers[7]\n        self.upconv3 = nn.ConvTranspose2d(2048, 1024, 2, 2); self.attn3 = AttentionGate(1024, 1024, 512); self.dec_conv3 = ConvBlock(2048, 1024)\n        self.upconv2 = nn.ConvTranspose2d(1024, 512, 2, 2); self.attn2 = AttentionGate(512, 512, 256); self.dec_conv2 = ConvBlock(1024, 512)\n        self.upconv1 = nn.ConvTranspose2d(512, 256, 2, 2); self.attn1 = AttentionGate(256, 256, 128); self.dec_conv1 = ConvBlock(512, 256)\n        self.upconv0 = nn.ConvTranspose2d(256, 64, 2, 2); self.attn0 = AttentionGate(64, 64, 32); self.dec_conv0 = ConvBlock(128, 64)\n        self.final_up = nn.ConvTranspose2d(64, 32, 2, 2); self.final_conv = nn.Conv2d(32, n_classes, 1)\n    def forward(self, x):\n        e0 = self.encoder0(x); e1 = self.encoder1(e0); e2 = self.encoder2(e1); e3 = self.encoder3(e2); e4 = self.encoder4(e3)\n        d3 = self.upconv3(e4); x3 = self.attn3(d3, e3); d3 = self.dec_conv3(torch.cat((x3, d3), 1))\n        d2 = self.upconv2(d3); x2 = self.attn2(d2, e2); d2 = self.dec_conv2(torch.cat((x2, d2), 1))\n        d1 = self.upconv1(d2); x1 = self.attn1(d1, e1); d1 = self.dec_conv1(torch.cat((x1, d1), 1))\n        d0 = self.upconv0(d1); x0 = self.attn0(d0, e0); d0 = self.dec_conv0(torch.cat((x0, d0), 1))\n        return self.final_conv(self.final_up(d0))\n\n# --- 6. LOADING SEGMENTATION MODELS ---\nmodels_loaded = False\nif test_df is not None and not test_df.empty:\n    try:\n        print(\"\\n--- Loading segmentation models... ---\")\n        model1 = UNetWithResNet50Encoder(n_classes=1).to(device)\n        model1.load_state_dict(torch.load(MODEL1_PATH, map_location=device))\n        model1.eval()\n        model2 = TransUNetPP(n_classes=1).to(device)\n        model2.load_state_dict(torch.load(MODEL2_PATH, map_location=device))\n        model2.eval()\n        model3 = AttentionUNet(n_classes=1).to(device)\n        model3.load_state_dict(torch.load(MODEL3_PATH, map_location=device))\n        model3.eval()\n        print(\"Segmentation models loaded successfully! âœ…\")\n        models_loaded = True\n    except Exception as e:\n        print(f\"Error loading segmentation model files (Check paths): {e}\")\n        models_loaded = False\nelse:\n    print(\"Skipping segmentation model loading as test data is unavailable.\")\n    \n# --- FOCAL LOSS IMPLEMENTATION ---\nclass FocalLoss(nn.Module):\n    def __init__(self, alpha=None, gamma=2., reduction='mean'):\n        super(FocalLoss, self).__init__()\n        self.gamma = gamma\n        self.alpha = alpha\n        self.reduction = reduction\n    def forward(self, input, target):\n        ce_loss = F.cross_entropy(input, target, reduction='none')\n        pt = torch.exp(-ce_loss)\n        loss = ((1 - pt) ** self.gamma * ce_loss)\n        if self.alpha is not None:\n            alpha_t = self.alpha[target.data.view(-1)]\n            loss = alpha_t * loss\n        if self.reduction == 'mean': return loss.mean()\n        else: return loss.sum()\n\n# =================================================================================\n# --- 7. CLASSIFICATION STAGE (HYBRID MODEL) ---\n# =================================================================================\n\nprint(\"\\n\\n--- Starting Classification Stage with Hybrid Model ---\")\nif not models_loaded or test_df.empty:\n    print(\"Skipping classification stage due to errors or missing data.\")\nelse:\n    try:\n        # --- 7.1 Generate Segmentation Predictions ---\n        print(\"\\nGenerating ensemble segmentation predictions...\")\n        train_seg_dataset = SegmentationInputDataset(train_df)\n        val_seg_dataset = SegmentationInputDataset(val_df)\n        test_seg_dataset = SegmentationInputDataset(test_df)\n\n        def get_segmentation_predictions(dataset, desc):\n            loader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=2)\n            all_preds = []\n            with torch.no_grad():\n                for images in tqdm(loader, desc=desc):\n                    images = images.to(device)\n                    # Ensemble predictions\n                    p1 = model1(images); p2 = model2(images); p3 = model3(images)\n                    ensembled = torch.sigmoid(torch.mean(torch.stack([p1, p2, p3]), dim=0))\n                    all_preds.append((ensembled > 0.5).float().cpu())\n            return torch.cat(all_preds)\n\n        train_pred_masks = get_segmentation_predictions(train_seg_dataset, \"Generating Train Masks\")\n        val_pred_masks = get_segmentation_predictions(val_seg_dataset, \"Generating Val Masks\")\n        test_pred_masks = get_segmentation_predictions(test_seg_dataset, \"Generating Test Masks\")\n        print(\"Segmentation predictions generated successfully.\")\n\n        # --- 7.2 Feature Engineering from Masks ---\n        print(\"\\nEngineering shape features from masks...\")\n\n        def get_shape_features(mask_tensor):\n            feature_df = pd.DataFrame()\n            for i in tqdm(range(len(mask_tensor)), desc=\"Calculating Features\"):\n                mask = mask_tensor[i, 0].numpy().astype(np.uint8)\n                # Ensure mask is 2D and binary\n                if mask.ndim == 3: mask = mask[:, :, 0] \n                mask = (mask > 0).astype(np.uint8)\n                \n                # Check for empty mask\n                if np.sum(mask) == 0:\n                    props_df = pd.DataFrame({'area': [0], 'perimeter': [0], 'solidity': [0], 'eccentricity': [0]})\n                else:\n                    try:\n                        # Use a small region minimum size to prevent issues with single pixels\n                        props = regionprops_table(mask, properties=('area', 'perimeter', 'solidity', 'eccentricity'))\n                        props_df = pd.DataFrame(props).head(1) # Take only the largest region if multiple exist\n                    except:\n                        props_df = pd.DataFrame({'area': [0], 'perimeter': [0], 'solidity': [0], 'eccentricity': [0]})\n                        \n                props_df['circularity'] = (4 * np.pi * props_df['area']) / (props_df['perimeter'] ** 2 + 1e-6)\n                feature_df = pd.concat([feature_df, props_df.head(1)], ignore_index=True)\n            feature_df.fillna(0, inplace=True)\n            return feature_df\n\n        train_features_df = get_shape_features(train_pred_masks)\n        val_features_df = get_shape_features(val_pred_masks)\n        test_features_df = get_shape_features(test_pred_masks)\n\n        # The shape features are: area, perimeter, solidity, eccentricity, circularity (5 features)\n        n_shape_features = train_features_df.shape[1] \n        scaler = StandardScaler()\n        train_features = scaler.fit_transform(train_features_df)\n        val_features = scaler.transform(val_features_df)\n        test_features = scaler.transform(test_features_df)\n        \n        # --- 7.3 Prepare Data and Datasets ---\n        train_transform = A.Compose([\n            A.Resize(height=IMAGE_SIZE[0], width=IMAGE_SIZE[1]), A.HorizontalFlip(p=0.5),\n            A.ShiftScaleRotate(shift_limit=0.05, scale_limit=0.05, rotate_limit=10, p=0.5),\n            A.RandomBrightnessContrast(p=0.3), A.GaussNoise(p=0.2), A.OpticalDistortion(p=0.2),\n            A.CoarseDropout(max_holes=8, max_height=8, max_width=8, p=0.2),\n            A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]), ToTensorV2()\n        ])\n        eval_transform = A.Compose([\n            A.Resize(height=IMAGE_SIZE[0], width=IMAGE_SIZE[1]), \n            A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]), ToTensorV2()\n        ])\n        minority_transform = A.Compose([ # More aggressive augmentation\n            A.Resize(height=IMAGE_SIZE[0], width=IMAGE_SIZE[1]), A.HorizontalFlip(p=0.8), A.VerticalFlip(p=0.5),\n            A.ShiftScaleRotate(shift_limit=0.1, scale_limit=0.1, rotate_limit=20, p=0.8),\n            A.RandomBrightnessContrast(brightness_limit=0.3, contrast_limit=0.3, p=0.5),\n            A.GaussNoise(p=0.3), A.OpticalDistortion(p=0.3),\n            A.CoarseDropout(max_holes=16, max_height=16, max_width=16, p=0.3),\n            A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]), ToTensorV2()\n        ])\n\n        class HybridDataset(Dataset):\n            def __init__(self, dataframe, pred_masks, shape_features, transform, minority_classes=None, minority_transform=None):\n                self.df = dataframe.reset_index(drop=True)\n                self.masks = pred_masks\n                self.features = shape_features\n                self.transform = transform\n                self.minority_classes = minority_classes if minority_classes else []\n                self.minority_transform = minority_transform if minority_transform else transform\n\n            def __len__(self): return len(self.df)\n\n            def __getitem__(self, idx):\n                row = self.df.iloc[idx]\n                image = cv2.imread(row['image_file_path'])\n                image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n                label = row['class_label']\n                \n                # Conditional augmentation\n                if label in self.minority_classes and self.minority_transform:\n                    transformed = self.minority_transform(image=image)\n                else:\n                    transformed = self.transform(image=image)\n\n                img_tensor = transformed['image']\n                \n                # Apply mask to the image (Liver only)\n                # Ensure mask is the right size/shape for element-wise multiplication\n                pred_mask = self.masks[idx].to(img_tensor.device) \n                masked_image = img_tensor * pred_mask\n                \n                shape_feats = torch.tensor(self.features[idx], dtype=torch.float)\n                label = torch.tensor(label, dtype=torch.long)\n                return masked_image, shape_feats, label\n\n        # Use setting from previous best run: only 'Severe' (2) gets strong augmentation\n        minority_classes_list = [2] \n        train_cls_dataset = HybridDataset(\n            train_df, train_pred_masks, train_features, \n            transform=train_transform, minority_classes=minority_classes_list, minority_transform=minority_transform\n        )\n        val_cls_dataset = HybridDataset(val_df, val_pred_masks, val_features, eval_transform)\n        test_cls_dataset = HybridDataset(test_df, test_pred_masks, test_features, eval_transform)\n        \n        # WeightedRandomSampler for class balancing\n        class_counts = train_df['class_label'].value_counts().to_dict()\n        num_samples = len(train_df)\n        class_weights_sampler = {i: num_samples / class_counts.get(i, 1) for i in range(NUM_CLASSES)}\n        sample_weights = np.array([class_weights_sampler[t] for t in train_df['class_label'].values])\n        sampler = torch.utils.data.WeightedRandomSampler(\n            weights=torch.from_numpy(sample_weights).double(), num_samples=len(sample_weights), replacement=True\n        )\n\n        train_cls_loader = DataLoader(train_cls_dataset, batch_size=BATCH_SIZE, sampler=sampler, num_workers=2, pin_memory=True)\n        val_cls_loader = DataLoader(val_cls_dataset, BATCH_SIZE, shuffle=False, num_workers=2, pin_memory=True)\n        test_cls_loader = DataLoader(test_cls_dataset, BATCH_SIZE, shuffle=False, num_workers=2, pin_memory=True)\n        print(\"\\nHybrid datasets and loaders are ready with WeightedRandomSampler. ğŸ“Š\")\n\n        # --- 7.4 HYBRID MODEL ARCHITECTURE (Dynamic Encoder) ---\n        \n        # PLACEHOLDER MODELS (since external weights/complex definitions are unavailable)\n        class CoAnNetFeatureExtractor(nn.Module):\n            # CoAnNet is complex; we use a simpler block with the expected output dimension\n            def __init__(self):\n                super().__init__()\n                self.conv = nn.Conv2d(3, 512, kernel_size=3, padding=1)\n                self.pool = nn.AdaptiveAvgPool2d((1, 1))\n                self.identity = nn.Identity()\n            def forward(self, x):\n                x = F.relu(self.conv(x))\n                return self.identity(self.pool(x).flatten(1))\n\n        class SimCLRResNet50(nn.Module):\n            # SimCLR is ResNet50 with special weights; we use a standard pre-trained one\n            def __init__(self):\n                super().__init__()\n                self.base = models.resnet50(weights=models.ResNet50_Weights.IMAGENET1K_V1)\n                self.base.fc = nn.Identity()\n            def forward(self, x):\n                return self.base(x)\n\n\n        class HybridClassifier(nn.Module):\n            def __init__(self, cnn_name: str, n_classes=3, n_shape_features=5):\n                super().__init__()\n                \n                cnn_feature_dim = 0\n                \n                if cnn_name == 'resnet34':\n                    self.cnn_base = models.resnet34(weights=models.ResNet34_Weights.DEFAULT)\n                    cnn_feature_dim = self.cnn_base.fc.in_features\n                    self.cnn_base.fc = nn.Identity()\n                elif cnn_name == 'resnet50' or cnn_name == 'ResNet50_GAP':\n                    # ResNet50 and ResNet50_GAP are the same in feature extraction layer, GAP is handled by AdaptiveAvgPool2d.\n                    self.cnn_base = models.resnet50(weights=models.ResNet50_Weights.DEFAULT)\n                    cnn_feature_dim = self.cnn_base.fc.in_features\n                    self.cnn_base.fc = nn.Identity()\n                elif cnn_name == 'densenet121':\n                    self.cnn_base = models.densenet121(weights=models.DenseNet121_Weights.DEFAULT)\n                    cnn_feature_dim = self.cnn_base.classifier.in_features\n                    self.cnn_base.classifier = nn.Identity()\n                elif cnn_name == 'EfficientNet_B0':\n                    self.cnn_base = models.efficientnet_b0(weights=models.EfficientNet_B0_Weights.DEFAULT)\n                    cnn_feature_dim = self.cnn_base.classifier[1].in_features\n                    self.cnn_base.classifier = nn.Identity() # Remove the default classifier\n                elif cnn_name == 'SimCLR_ResNet50':\n                    self.cnn_base = SimCLRResNet50() # Custom/Pre-loaded weights are required for true SimCLR\n                    cnn_feature_dim = 2048\n                elif cnn_name == 'CoAnNet':\n                    self.cnn_base = CoAnNetFeatureExtractor() # Placeholder\n                    cnn_feature_dim = 512 # Placeholder dimension\n                else:\n                    raise ValueError(f\"Unknown CNN name: {cnn_name}\")\n                    \n                print(f\"Loaded {cnn_name} encoder (Dim: {cnn_feature_dim})\")\n\n                self.tabular_mlp = nn.Sequential(\n                    nn.Linear(n_shape_features, 64), nn.ReLU(), nn.Dropout(0.4),\n                    nn.Linear(64, 32), nn.ReLU(), nn.Dropout(0.4)\n                )\n                self.fusion_classifier = nn.Sequential(\n                    nn.Linear(cnn_feature_dim + 32, 256), nn.ReLU(), nn.Dropout(0.5),\n                    nn.Linear(256, n_classes)\n                )\n                \n            def forward(self, image, shape_features):\n                cnn_features = self.cnn_base(image)\n                # Ensure 1D output after CNN\n                if cnn_features.ndim > 2:\n                    # Adaptive Avg Pool for any remaining spatial dimensions (e.g., in ResNet/EfficientNet backbone output)\n                    cnn_features = F.adaptive_avg_pool2d(cnn_features, (1, 1)).flatten(1) \n                    \n                tabular_features = self.tabular_mlp(shape_features)\n                combined_features = torch.cat([cnn_features, tabular_features], dim=1)\n                output = self.fusion_classifier(combined_features)\n                return output\n\n        # --- 7.5 TRAIN AND EVALUATE FUNCTION PER MODEL ---\n        def train_and_evaluate_single_model(cnn_name: str, train_loader, val_loader, test_loader, class_weights, n_shape_features, device, num_epochs=NUM_EPOCHS, patience=10):\n            \"\"\"Trains and evaluates a single hybrid model.\"\"\"\n            print(f\"\\n=============================================\")\n            print(f\"ğŸš€ Starting Training for Model: {cnn_name}\")\n            print(f\"=============================================\")\n            \n            cls_model = HybridClassifier(cnn_name=cnn_name, n_classes=NUM_CLASSES, n_shape_features=n_shape_features).to(device)\n            \n            # Use gamma=2.0 as it gave the best result so far\n            criterion = FocalLoss(alpha=class_weights, gamma=2.0)\n            \n            optimizer = torch.optim.AdamW(cls_model.parameters(), lr=1e-4, weight_decay=1e-5)\n            scheduler = ReduceLROnPlateau(optimizer, mode='max', factor=0.2, patience=5, verbose=False)\n            \n            epochs_no_improve = 0; best_val_accuracy = 0.0\n            model_save_path = f'best_classifier_model_{cnn_name}.pth'\n            \n            for epoch in range(num_epochs):\n                cls_model.train()\n                running_loss = 0.0\n                for images, shapes, labels in train_loader:\n                    images, shapes, labels = images.to(device), shapes.to(device), labels.to(device)\n                    optimizer.zero_grad()\n                    outputs = cls_model(images, shapes)\n                    loss = criterion(outputs, labels); loss.backward(); optimizer.step()\n                    running_loss += loss.item()\n                \n                cls_model.eval()\n                val_corrects = 0\n                with torch.no_grad():\n                    for images, shapes, labels in val_loader:\n                        images, shapes, labels = images.to(device), shapes.to(device), labels.to(device)\n                        outputs = cls_model(images, shapes); _, preds = torch.max(outputs, 1)\n                        val_corrects += torch.sum(preds == labels.data)\n\n                val_accuracy = val_corrects.double() / len(val_loader.dataset)\n                scheduler.step(val_accuracy)\n\n                # Early stopping logic\n                if val_accuracy > best_val_accuracy:\n                    best_val_accuracy = val_accuracy\n                    torch.save(cls_model.state_dict(), model_save_path)\n                    epochs_no_improve = 0\n                else:\n                    epochs_no_improve += 1\n                    \n                if epochs_no_improve >= patience:\n                    print(f\"â¹ï¸ Early stopping triggered for {cnn_name} at epoch {epoch+1}.\")\n                    break\n                    \n            print(f\"Final Best Validation Accuracy for {cnn_name}: {best_val_accuracy:.4f}\")\n\n            # Final Test Evaluation\n            try:\n                cls_model.load_state_dict(torch.load(model_save_path, map_location=device))\n            except Exception as e:\n                 print(f\"âš ï¸ Could not load best weights for {cnn_name}. Using current weights for testing. Error: {e}\")\n            cls_model.eval()\n            all_labels, all_preds = [], []\n            with torch.no_grad():\n                for images, shapes, labels in test_loader:\n                    images, shapes, labels = images.to(device), shapes.to(device), labels.to(device)\n                    outputs = cls_model(images, shapes); _, preds = torch.max(outputs, 1)\n                    all_labels.extend(labels.cpu().numpy()); all_preds.extend(preds.cpu().numpy())\n            \n            report = classification_report(all_labels, all_preds, target_names=CLASS_NAMES, zero_division=0, output_dict=True)\n            return np.array(all_preds), report\n\n        # --- 7.6 MASTER EXECUTION AND COMPARISON ---\n        \n        # ADD ALL MODELS TO BE EVALUATED\n        model_list = [\n            'resnet34', \n            'resnet50', \n            'densenet121', \n            'EfficientNet_B0',\n            'ResNet50_GAP',      # Standard ResNet50 encoder using GAP\n            'SimCLR_ResNet50',   # ResNet50 with placeholder SimCLR weights\n            'CoAnNet'            # Placeholder architecture\n        ]\n        results = {} \n\n        for model_name in model_list:\n            preds, report = train_and_evaluate_single_model(\n                cnn_name=model_name,\n                train_loader=train_cls_loader, val_loader=val_cls_loader, test_loader=test_cls_loader,\n                class_weights=class_weights, n_shape_features=n_shape_features, device=device\n            )\n            results[model_name] = {'preds': preds, 'report': report}\n\n        # --- 7.7 FINAL ENSEMBLE SELECTION AND REPORT ---\n        print(\"\\n=============================================\")\n        print(\"ğŸ† FINAL RESULT SELECTION (PER-CLASS BEST RECALL)\")\n        print(\"=============================================\")\n\n        # 1. Get ground truth labels\n        all_labels = np.concatenate([labels.cpu().numpy() for _, _, labels in test_cls_loader])\n        final_preds = np.zeros_like(all_labels)\n        \n        best_model_per_class = {}\n        \n        # 2. Determine the best model for each class based on 'recall' (accuracy per class)\n        comparison_data = []\n        for i, class_name in enumerate(CLASS_NAMES):\n            best_recall = -1.0\n            best_model = None\n            \n            row = {'Class': class_name}\n            for model_name, res in results.items():\n                recall = res['report'].get(class_name, {}).get('recall', 0.0) # Handle missing class gracefully\n                row[model_name] = f'{recall:.4f}'\n                \n                if recall > best_recall:\n                    best_recall = recall\n                    best_model = model_name\n                    \n            best_model_per_class[i] = best_model\n            row['Best Model'] = best_model\n            row['Best Recall'] = f'{best_recall:.4f}'\n            comparison_data.append(row)\n            \n            # 3. Create the final combined prediction array (Per-Class Best Model Selection)\n            class_indices = np.where(all_labels == i)[0]\n            if best_model:\n                 final_preds[class_indices] = results[best_model]['preds'][class_indices]\n\n        # Print comparison table\n        comparison_df = pd.DataFrame(comparison_data)\n        print(\"\\n--- PER-CLASS RECALL COMPARISON ---\")\n        print(comparison_df.to_markdown(index=False))\n        \n        # Print Final Combined Report\n        print(\"\\n--- FINAL CLASSIFICATION REPORT (Combined Best Recall) ---\")\n        print(classification_report(all_labels, final_preds, target_names=CLASS_NAMES, zero_division=0))\n\n        # Confusion Matrix for the Combined Result\n        cm_final = confusion_matrix(all_labels, final_preds, labels=range(NUM_CLASSES))\n        disp_final = ConfusionMatrixDisplay(confusion_matrix=cm_final, display_labels=CLASS_NAMES)\n        fig, ax = plt.subplots(figsize=(8, 8))\n        disp_final.plot(ax=ax, cmap=plt.cm.Greens)\n        plt.title(\"Confusion Matrix (Best Per-Class Selection)\")\n        plt.show()\n\n    except Exception as e:\n        print(f\"\\nAn error occurred in the Classification Pipeline: {e}\")\n        import traceback\n        traceback.print_exc()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-11T18:00:52.987697Z","iopub.execute_input":"2025-11-11T18:00:52.988454Z","iopub.status.idle":"2025-11-11T20:00:12.840085Z","shell.execute_reply.started":"2025-11-11T18:00:52.988429Z","shell.execute_reply":"2025-11-11T20:00:12.839095Z"}},"outputs":[{"name":"stdout","text":"Using device: cuda âš™ï¸\n\n--- Creating DataFrames ---\nSuccessfully created dataframes: 5364 train, 674 val, 664 test.\nOriginal calculated weights: [0.82055989 0.8641856  1.60215054]\nBoosted 'Moderate' weight. New weights: [0.82055989 1.03702272 1.60215054]\n\nFinal Class Weights for Focal Loss: tensor([0.8206, 1.0370, 1.6022], device='cuda:0')\n\n--- Loading segmentation models... ---\nSegmentation models loaded successfully! âœ…\n\n\n--- Starting Classification Stage with Hybrid Model ---\n\nGenerating ensemble segmentation predictions...\n","output_type":"stream"},{"name":"stderr","text":"Generating Train Masks: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 336/336 [03:23<00:00,  1.65it/s]\nGenerating Val Masks: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 43/43 [00:26<00:00,  1.64it/s]\nGenerating Test Masks: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 42/42 [00:25<00:00,  1.62it/s]\n","output_type":"stream"},{"name":"stdout","text":"Segmentation predictions generated successfully.\n\nEngineering shape features from masks...\n","output_type":"stream"},{"name":"stderr","text":"Calculating Features: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5364/5364 [00:23<00:00, 227.17it/s]\nCalculating Features: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 674/674 [00:03<00:00, 222.23it/s]\nCalculating Features: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 664/664 [00:02<00:00, 223.70it/s]\n","output_type":"stream"},{"name":"stdout","text":"\nHybrid datasets and loaders are ready with WeightedRandomSampler. ğŸ“Š\n\n=============================================\nğŸš€ Starting Training for Model: resnet34\n=============================================\nLoaded resnet34 encoder (Dim: 512)\nâ¹ï¸ Early stopping triggered for resnet34 at epoch 14.\nFinal Best Validation Accuracy for resnet34: 0.7136\n\n=============================================\nğŸš€ Starting Training for Model: resnet50\n=============================================\nLoaded resnet50 encoder (Dim: 2048)\nâ¹ï¸ Early stopping triggered for resnet50 at epoch 14.\nFinal Best Validation Accuracy for resnet50: 0.7226\n\n=============================================\nğŸš€ Starting Training for Model: densenet121\n=============================================\nLoaded densenet121 encoder (Dim: 1024)\nâ¹ï¸ Early stopping triggered for densenet121 at epoch 15.\nFinal Best Validation Accuracy for densenet121: 0.7136\n\n=============================================\nğŸš€ Starting Training for Model: EfficientNet_B0\n=============================================\nLoaded EfficientNet_B0 encoder (Dim: 1280)\nâ¹ï¸ Early stopping triggered for EfficientNet_B0 at epoch 16.\nFinal Best Validation Accuracy for EfficientNet_B0: 0.6632\n\n=============================================\nğŸš€ Starting Training for Model: ResNet50_GAP\n=============================================\nLoaded ResNet50_GAP encoder (Dim: 2048)\nâ¹ï¸ Early stopping triggered for ResNet50_GAP at epoch 21.\nFinal Best Validation Accuracy for ResNet50_GAP: 0.7315\n\n=============================================\nğŸš€ Starting Training for Model: SimCLR_ResNet50\n=============================================\nLoaded SimCLR_ResNet50 encoder (Dim: 2048)\nâ¹ï¸ Early stopping triggered for SimCLR_ResNet50 at epoch 21.\nFinal Best Validation Accuracy for SimCLR_ResNet50: 0.7077\n\n=============================================\nğŸš€ Starting Training for Model: CoAnNet\n=============================================\nLoaded CoAnNet encoder (Dim: 512)\nâ¹ï¸ Early stopping triggered for CoAnNet at epoch 33.\nFinal Best Validation Accuracy for CoAnNet: 0.3858\n\n=============================================\nğŸ† FINAL RESULT SELECTION (PER-CLASS BEST RECALL)\n=============================================\n\n--- PER-CLASS RECALL COMPARISON ---\n| Class    |   resnet34 |   resnet50 |   densenet121 |   EfficientNet_B0 |   ResNet50_GAP |   SimCLR_ResNet50 |   CoAnNet | Best Model      |   Best Recall |\n|:---------|-----------:|-----------:|--------------:|------------------:|---------------:|------------------:|----------:|:----------------|--------------:|\n| Mild     |     0.5743 |     0.72   |        0.5086 |            0.64   |         0.6971 |            0.7371 |    0      | SimCLR_ResNet50 |        0.7371 |\n| Moderate |     0.5639 |     0.4737 |        0.2932 |            0.5338 |         0.4286 |            0.3534 |    0.3534 | resnet34        |        0.5639 |\n| Severe   |     0.3978 |     0.4475 |        0.7348 |            0.337  |         0.4088 |            0.5414 |    0.884  | CoAnNet         |        0.884  |\n\n--- FINAL CLASSIFICATION REPORT (Combined Best Recall) ---\n              precision    recall  f1-score   support\n\n        Mild       0.92      0.74      0.82       350\n    Moderate       0.47      0.56      0.52       133\n      Severe       0.71      0.88      0.79       181\n\n    accuracy                           0.74       664\n   macro avg       0.70      0.73      0.71       664\nweighted avg       0.77      0.74      0.75       664\n\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<Figure size 800x800 with 2 Axes>","image/png":"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\n"},"metadata":{}}],"execution_count":4},{"cell_type":"code","source":"import os\nimport torch\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom PIL import Image\nimport torch.nn.functional as F\nimport torch.nn as nn\nfrom torchvision import models, transforms\nfrom torch.utils.data import DataLoader, Dataset\nfrom tqdm import tqdm\nfrom sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay\nfrom sklearn.utils.class_weight import compute_class_weight\nfrom torch.optim.lr_scheduler import ReduceLROnPlateau\nimport albumentations as A\nfrom albumentations.pytorch import ToTensorV2\nimport cv2\nfrom skimage.measure import regionprops_table\nfrom sklearn.preprocessing import StandardScaler\nimport warnings\n\n# Suppress minor warnings for clean output\nwarnings.filterwarnings(\"ignore\", category=UserWarning)\n\n# --- 1. CONFIGURATION ---\n# NOTE: Replace these paths with actual working paths if running outside a Kaggle environment\nMODEL1_PATH = '/kaggle/input/ourensemble5/3URESNET/best_segmentation_model1.pth' # UNet with ResNet50\nMODEL2_PATH = '/kaggle/input/ourensemble5/3URESNET/best_transunetpp_model.pth' # TransUNetPP\nMODEL3_PATH = '/kaggle/input/ourensemble5/3URESNET/best_segmentation_model3.pth' # AttentionUNet\nMETADATA_PATH = '/kaggle/input/t2metadataaa/T2_age_gender_evaluation.csv'\nTEST_DIR = '/kaggle/input/dataa1/Cirrhosis_T2_2D/test'\nTRAIN_DIR = '/kaggle/input/dataa1/Cirrhosis_T2_2D/train'\nVAL_DIR = '/kaggle/input/dataa1/Cirrhosis_T2_2D/valid'\nIMAGE_SIZE = (224, 224)\nCLASS_NAMES = ['Mild', 'Moderate', 'Severe']\nNUM_CLASSES = len(CLASS_NAMES)\nNUM_EPOCHS = 50\nBATCH_SIZE = 16\n\n# --- 2. SETUP ---\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device} âš™ï¸\")\n\n# --- 3. DATAFRAME CREATION AND MERGE ---\nprint(\"\\n--- Creating DataFrames ---\")\ntry:\n    # Load and clean metadata\n    metadata_df = pd.read_csv(METADATA_PATH)\n    metadata_df.rename(columns={'Patient ID': 'ID', 'Radiological Evaluation': 'radiological_evaluation'}, inplace=True)\n    metadata_df.dropna(subset=['radiological_evaluation'], inplace=True)\n    metadata_df['radiological_evaluation'] = metadata_df['radiological_evaluation'].astype(int)\n    metadata_df['ID'] = metadata_df['ID'].astype(str)\n    class_label_map = {1: 0, 2: 1, 3: 2} # Mild=0, Moderate=1, Severe=2\n    metadata_df['class_label'] = metadata_df['radiological_evaluation'].map(class_label_map)\n    metadata_df.dropna(subset=['class_label'], inplace=True)\n    valid_ids = set(metadata_df['ID'].tolist())\n\n    # Helper function to create dataframe from file structure\n    def create_dataframe_from_ids(directory, allowed_ids):\n        data = []\n        if not os.path.exists(directory): return pd.DataFrame(data)\n        for folder_name in os.listdir(directory):\n            if folder_name in allowed_ids:\n                folder_path = os.path.join(directory, folder_name)\n                images_dir = os.path.join(folder_path, 'images')\n                if os.path.exists(images_dir):\n                    for image_file in os.listdir(images_dir):\n                        mask_path = os.path.join(folder_path, 'masks', image_file)\n                        if os.path.exists(mask_path):\n                            image_path = os.path.join(images_dir, image_file)\n                            data.append((folder_name, image_path, mask_path))\n        return pd.DataFrame(data, columns=['ID', 'image_file_path', 'mask_file_path'])\n\n    # Merge dataframes\n    train_df = pd.merge(create_dataframe_from_ids(TRAIN_DIR, valid_ids), metadata_df, on='ID')\n    val_df = pd.merge(create_dataframe_from_ids(VAL_DIR, valid_ids), metadata_df, on='ID')\n    test_df = pd.merge(create_dataframe_from_ids(TEST_DIR, valid_ids), metadata_df, on='ID')\n\n    if train_df.empty or val_df.empty or test_df.empty:\n        raise ValueError(\"One or more dataframes are empty. Check your file paths and metadata.\")\n    \n    print(f\"Successfully created dataframes: {len(train_df)} train, {len(val_df)} val, {len(test_df)} test.\")\n    \n    # *** UPDATED HERE: Define BOTH weight strategies ***\n    # 1. Base calculated weights\n    base_weights_array = compute_class_weight('balanced', classes=np.unique(train_df['class_label'].values), y=train_df['class_label'].values)\n    print(f\"Original calculated weights: {base_weights_array}\")\n\n    # 2. Strategy A: Conservative Boost (1.2)\n    weights_array_1_2 = np.copy(base_weights_array)\n    weights_array_1_2[1] = weights_array_1_2[1] * 1.2\n    class_weights_boost_1_2 = torch.tensor(weights_array_1_2, dtype=torch.float).to(device)\n    print(f\"Weights (Boost 1.2): {class_weights_boost_1_2}\")\n\n    # 3. Strategy B: Aggressive Boost (1.5)\n    weights_array_1_5 = np.copy(base_weights_array)\n    weights_array_1_5[1] = weights_array_1_5[1] * 1.5\n    class_weights_boost_1_5 = torch.tensor(weights_array_1_5, dtype=torch.float).to(device)\n    print(f\"Weights (Boost 1.5): {class_weights_boost_1_5}\")\n\n\nexcept Exception as e:\n    print(f\"Error during data setup: {e}\")\n    test_df = None\n\n# --- 4. DATASET FOR INITIAL SEGMENTATION ---\nclass SegmentationInputDataset(Dataset):\n    def __init__(self, dataframe):\n        self.dataframe = dataframe\n        # Standard normalization for CNNs (e.g., ImageNet weights)\n        self.transform = transforms.Compose([\n            transforms.Resize(IMAGE_SIZE, transforms.InterpolationMode.BILINEAR),\n            transforms.ToTensor(),\n            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n        ])\n    def __len__(self): return len(self.dataframe)\n    def __getitem__(self, idx):\n        row = self.dataframe.iloc[idx]\n        image_pil = Image.open(row['image_file_path']).convert('RGB')\n        return self.transform(image_pil)\n\n# --- 5. MODEL ARCHITECTURE DEFINITIONS (Segmentation Models) ---\n# NOTE: Using the user's provided segmentation model definitions\n\nclass ConvBlock(nn.Module):\n    def __init__(self, in_ch, out_ch):\n        super().__init__()\n        self.conv = nn.Sequential(\n            nn.Conv2d(in_ch, out_ch, 3, 1, 1, bias=False), nn.BatchNorm2d(out_ch), nn.ReLU(inplace=True),\n            nn.Conv2d(out_ch, out_ch, 3, 1, 1, bias=False), nn.BatchNorm2d(out_ch), nn.ReLU(inplace=True))\n    def forward(self, x): return self.conv(x)\n\nclass DecoderBlock(nn.Module):\n    def __init__(self, in_channels, skip_channels, out_channels):\n        super().__init__()\n        self.conv1 = nn.Conv2d(in_channels + skip_channels, out_channels, kernel_size=3, padding=1, bias=False)\n        self.bn1 = nn.BatchNorm2d(out_channels)\n        self.relu = nn.ReLU(inplace=True)\n        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1, bias=False)\n        self.bn2 = nn.BatchNorm2d(out_channels)\n    def forward(self, x, skip_connection):\n        x = F.interpolate(x, scale_factor=2, mode='bilinear', align_corners=True)\n        x = torch.cat([x, skip_connection], dim=1)\n        x = self.relu(self.bn1(self.conv1(x)))\n        x = self.relu(self.bn2(self.conv2(x)))\n        return x\n\nclass UNetWithResNet50Encoder(nn.Module):\n    def __init__(self, n_classes=1):\n        super().__init__()\n        base_model = models.resnet50(weights=None)\n        base_layers = list(base_model.children())\n        self.encoder0, self.encoder1 = nn.Sequential(*base_layers[:3]), nn.Sequential(*base_layers[3:5])\n        self.encoder2, self.encoder3, self.encoder4 = base_layers[5], base_layers[6], base_layers[7]\n        self.decoder3 = DecoderBlock(2048, 1024, 512)\n        self.decoder2 = DecoderBlock(512, 512, 256)\n        self.decoder1 = DecoderBlock(256, 256, 128)\n        self.decoder0 = DecoderBlock(128, 64, 64)\n        self.final_conv = nn.Conv2d(64, n_classes, kernel_size=1)\n    \n    # *** UPDATED FORWARD PASS ***\n    def forward(self, x):\n        e0 = self.encoder0(x); e1 = self.encoder1(e0); e2 = self.encoder2(e1); e3 = self.encoder3(e2); e4 = self.encoder4(e3)\n        d3 = self.decoder3(e4, e3); d2 = self.decoder2(d3, e2); d1 = self.decoder1(d2, e1); d0 = self.decoder0(d1, e0)\n        \n        # Swapped order to match model2 (TransUNetPP) and avoid the interpolate bug\n        out = self.final_conv(d0) \n        out = F.interpolate(out, scale_factor=2, mode='bilinear', align_corners=True)\n        return out\n\nclass TransUNetPP(nn.Module):\n    def __init__(self, n_classes=1, img_dim=224, vit_dim=768, vit_depth=12, vit_heads=12):\n        super().__init__()\n        base_model = models.resnet50(weights=None)\n        base_layers = list(base_model.children())\n        self.encoder0, self.encoder1 = nn.Sequential(*base_layers[:3]), nn.Sequential(*base_layers[3:5])\n        self.encoder2, self.encoder3, self.encoder4 = base_layers[5], base_layers[6], base_layers[7]\n        num_patches, self.patch_dim = (img_dim // 32) ** 2, 2048\n        self.pos_embedding = nn.Parameter(torch.randn(1, num_patches, vit_dim))\n        self.patch_to_embedding = nn.Linear(self.patch_dim, vit_dim)\n        transformer_layer = nn.TransformerEncoderLayer(d_model=vit_dim, nhead=vit_heads, dim_feedforward=vit_dim * 4, batch_first=True)\n        self.transformer_encoder = nn.TransformerEncoder(transformer_layer, num_layers=vit_depth)\n        self.transformer_output_to_conv = nn.Sequential(nn.Linear(vit_dim, self.patch_dim), nn.LayerNorm(self.patch_dim))\n        d_ch = {'d0': 64, 'd1': 128, 'd2': 256, 'd3': 512, 'd4': 1024}\n        self.upsample = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n        self.X_0_0 = ConvBlock(64, d_ch['d0']); self.X_1_0 = ConvBlock(256, d_ch['d1'])\n        self.X_0_1 = ConvBlock(d_ch['d0'] + d_ch['d1'], d_ch['d0'])\n        self.X_2_0 = ConvBlock(512, d_ch['d2']); self.X_1_1 = ConvBlock(d_ch['d1'] + d_ch['d2'], d_ch['d1'])\n        self.X_0_2 = ConvBlock(d_ch['d0'] * 2 + d_ch['d1'], d_ch['d0'])\n        self.X_3_0 = ConvBlock(1024, d_ch['d3']); self.X_2_1 = ConvBlock(d_ch['d2'] + d_ch['d3'], d_ch['d2'])\n        self.X_1_2 = ConvBlock(d_ch['d1'] * 2 + d_ch['d2'], d_ch['d1'])\n        self.X_0_3 = ConvBlock(d_ch['d0'] * 3 + d_ch['d1'], d_ch['d0'])\n        self.X_4_0 = ConvBlock(2048, d_ch['d4']); self.X_3_1 = ConvBlock(d_ch['d3'] + d_ch['d4'], d_ch['d3'])\n        self.X_2_2 = ConvBlock(d_ch['d2'] * 2 + d_ch['d3'], d_ch['d2'])\n        self.X_1_3 = ConvBlock(d_ch['d1'] * 3 + d_ch['d2'], d_ch['d1'])\n        self.X_0_4 = ConvBlock(d_ch['d0'] * 4 + d_ch['d1'], d_ch['d0'])\n        self.final_conv = nn.Conv2d(d_ch['d0'], n_classes, kernel_size=1)\n    def forward(self, x):\n        e0 = self.encoder0(x); e1 = self.encoder1(e0); e2 = self.encoder2(e1); e3 = self.encoder3(e2); e4 = self.encoder4(e3)\n        bs, _, h, w = e4.shape\n        trans_in = self.patch_to_embedding(e4.flatten(2).transpose(1, 2)) + self.pos_embedding\n        trans_out = self.transformer_output_to_conv(self.transformer_encoder(trans_in)).transpose(1, 2).view(bs, self.patch_dim, h, w)\n        x0_0 = self.X_0_0(e0); x1_0 = self.X_1_0(e1); x0_1 = self.X_0_1(torch.cat([x0_0, self.upsample(x1_0)], 1))\n        x2_0 = self.X_2_0(e2); x1_1 = self.X_1_1(torch.cat([x1_0, self.upsample(x2_0)], 1)); x0_2 = self.X_0_2(torch.cat([x0_0, x0_1, self.upsample(x1_1)], 1))\n        x3_0 = self.X_3_0(e3); x2_1 = self.X_2_1(torch.cat([x2_0, self.upsample(x3_0)], 1)); x1_2 = self.X_1_2(torch.cat([x1_0, x1_1, self.upsample(x2_1)], 1))\n        x0_3 = self.X_0_3(torch.cat([x0_0, x0_1, x0_2, self.upsample(x1_2)], 1))\n        x4_0 = self.X_4_0(trans_out); x3_1 = self.X_3_1(torch.cat([x3_0, self.upsample(x4_0)], 1)); x2_2 = self.X_2_2(torch.cat([x2_0, x2_1, self.upsample(x3_1)], 1))\n        x1_3 = self.X_1_3(torch.cat([x1_0, x1_1, x1_2, self.upsample(x2_2)], 1)); x0_4 = self.X_0_4(torch.cat([x0_0, x0_1, x0_2, x0_3, self.upsample(x1_3)], 1))\n        return F.interpolate(self.final_conv(x0_4), scale_factor=2, mode='bilinear', align_corners=True)\n\nclass AttentionGate(nn.Module):\n    def __init__(self, F_g, F_l, F_int):\n        super().__init__(); self.W_g = nn.Sequential(nn.Conv2d(F_g, F_int, 1), nn.BatchNorm2d(F_int))\n        self.W_x = nn.Sequential(nn.Conv2d(F_l, F_int, 1), nn.BatchNorm2d(F_int))\n        self.psi = nn.Sequential(nn.Conv2d(F_int, 1, 1), nn.BatchNorm2d(1), nn.Sigmoid()); self.relu = nn.ReLU(inplace=True)\n    def forward(self, g, x): psi = self.relu(self.W_g(g) + self.W_x(x)); return x * self.psi(psi)\n\nclass AttentionUNet(nn.Module):\n    def __init__(self, n_classes=1):\n        super().__init__()\n        base = models.resnet50(weights=None); layers = list(base.children())\n        self.encoder0, self.encoder1 = nn.Sequential(*layers[:3]), nn.Sequential(*layers[3:5])\n        self.encoder2, self.encoder3, self.encoder4 = layers[5], layers[6], layers[7]\n        self.upconv3 = nn.ConvTranspose2d(2048, 1024, 2, 2); self.attn3 = AttentionGate(1024, 1024, 512); self.dec_conv3 = ConvBlock(2048, 1024)\n        self.upconv2 = nn.ConvTranspose2d(1024, 512, 2, 2); self.attn2 = AttentionGate(512, 512, 256); self.dec_conv2 = ConvBlock(1024, 512)\n        self.upconv1 = nn.ConvTranspose2d(512, 256, 2, 2); self.attn1 = AttentionGate(256, 256, 128); self.dec_conv1 = ConvBlock(512, 256)\n        self.upconv0 = nn.ConvTranspose2d(256, 64, 2, 2); self.attn0 = AttentionGate(64, 64, 32); self.dec_conv0 = ConvBlock(128, 64)\n        self.final_up = nn.ConvTranspose2d(64, 32, 2, 2); self.final_conv = nn.Conv2d(32, n_classes, 1)\n    def forward(self, x):\n        e0 = self.encoder0(x); e1 = self.encoder1(e0); e2 = self.encoder2(e1); e3 = self.encoder3(e2); e4 = self.encoder4(e3)\n        d3 = self.upconv3(e4); x3 = self.attn3(d3, e3); d3 = self.dec_conv3(torch.cat((x3, d3), 1))\n        d2 = self.upconv2(d3); x2 = self.attn2(d2, e2); d2 = self.dec_conv2(torch.cat((x2, d2), 1))\n        d1 = self.upconv1(d2); x1 = self.attn1(d1, e1); d1 = self.dec_conv1(torch.cat((x1, d1), 1))\n        d0 = self.upconv0(d1); x0 = self.attn0(d0, e0); d0 = self.dec_conv0(torch.cat((x0, d0), 1))\n        return self.final_conv(self.final_up(d0))\n\n# --- 6. LOADING SEGMENTATION MODELS ---\nmodels_loaded = False\nif test_df is not None and not test_df.empty:\n    try:\n        print(\"\\n--- Loading segmentation models... ---\")\n        model1 = UNetWithResNet50Encoder(n_classes=1).to(device)\n        model1.load_state_dict(torch.load(MODEL1_PATH, map_location=device))\n        model1.eval()\n        model2 = TransUNetPP(n_classes=1).to(device)\n        model2.load_state_dict(torch.load(MODEL2_PATH, map_location=device))\n        model2.eval()\n        model3 = AttentionUNet(n_classes=1).to(device)\n        model3.load_state_dict(torch.load(MODEL3_PATH, map_location=device))\n        model3.eval()\n        print(\"Segmentation models loaded successfully! âœ…\")\n        models_loaded = True\n    except Exception as e:\n        print(f\"Error loading segmentation model files (Check paths): {e}\")\n        models_loaded = False\nelse:\n    print(\"Skipping segmentation model loading as test data is unavailable.\")\n    \n# --- FOCAL LOSS IMPLEMENTATION ---\nclass FocalLoss(nn.Module):\n    def __init__(self, alpha=None, gamma=2., reduction='mean'):\n        super(FocalLoss, self).__init__()\n        self.gamma = gamma\n        self.alpha = alpha\n        self.reduction = reduction\n    def forward(self, input, target):\n        ce_loss = F.cross_entropy(input, target, reduction='none')\n        pt = torch.exp(-ce_loss)\n        loss = ((1 - pt) ** self.gamma * ce_loss)\n        if self.alpha is not None:\n            alpha_t = self.alpha[target.data.view(-1)]\n            loss = alpha_t * loss\n        if self.reduction == 'mean': return loss.mean()\n        else: return loss.sum()\n\n# =================================================================================\n# --- 7. CLASSIFICATION STAGE (HYBRID MODEL) ---\n# =================================================================================\n\nprint(\"\\n\\n--- Starting Classification Stage with Hybrid Model ---\")\nif not models_loaded or test_df.empty:\n    print(\"Skipping classification stage due to errors or missing data.\")\nelse:\n    try:\n        # --- 7.1 Generate Segmentation Predictions ---\n        print(\"\\nGenerating ensemble segmentation predictions...\")\n        train_seg_dataset = SegmentationInputDataset(train_df)\n        val_seg_dataset = SegmentationInputDataset(val_df)\n        test_seg_dataset = SegmentationInputDataset(test_df)\n\n        def get_segmentation_predictions(dataset, desc):\n            loader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=2)\n            all_preds = []\n            with torch.no_grad():\n                for images in tqdm(loader, desc=desc):\n                    images = images.to(device)\n                    # Ensemble predictions\n                    p1 = model1(images); p2 = model2(images); p3 = model3(images)\n                    ensembled = torch.sigmoid(torch.mean(torch.stack([p1, p2, p3]), dim=0))\n                    all_preds.append((ensembled > 0.5).float().cpu())\n            return torch.cat(all_preds)\n\n        train_pred_masks = get_segmentation_predictions(train_seg_dataset, \"Generating Train Masks\")\n        val_pred_masks = get_segmentation_predictions(val_seg_dataset, \"Generating Val Masks\")\n        test_pred_masks = get_segmentation_predictions(test_seg_dataset, \"Generating Test Masks\")\n        print(\"Segmentation predictions generated successfully.\")\n\n        # --- 7.2 Feature Engineering from Masks ---\n        print(\"\\nEngineering shape features from masks...\")\n\n        def get_shape_features(mask_tensor):\n            feature_df = pd.DataFrame()\n            for i in tqdm(range(len(mask_tensor)), desc=\"Calculating Features\"):\n                mask = mask_tensor[i, 0].numpy().astype(np.uint8)\n                # Ensure mask is 2D and binary\n                if mask.ndim == 3: mask = mask[:, :, 0] \n                mask = (mask > 0).astype(np.uint8)\n                \n                # Check for empty mask\n                if np.sum(mask) == 0:\n                    props_df = pd.DataFrame({'area': [0], 'perimeter': [0], 'solidity': [0], 'eccentricity': [0]})\n                else:\n                    try:\n                        # Use a small region minimum size to prevent issues with single pixels\n                        props = regionprops_table(mask, properties=('area', 'perimeter', 'solidity', 'eccentricity'))\n                        props_df = pd.DataFrame(props).head(1) # Take only the largest region if multiple exist\n                    except:\n                        props_df = pd.DataFrame({'area': [0], 'perimeter': [0], 'solidity': [0], 'eccentricity': [0]})\n                        \n                props_df['circularity'] = (4 * np.pi * props_df['area']) / (props_df['perimeter'] ** 2 + 1e-6)\n                feature_df = pd.concat([feature_df, props_df.head(1)], ignore_index=True)\n            feature_df.fillna(0, inplace=True)\n            return feature_df\n\n        train_features_df = get_shape_features(train_pred_masks)\n        val_features_df = get_shape_features(val_pred_masks)\n        test_features_df = get_shape_features(test_pred_masks)\n\n        # The shape features are: area, perimeter, solidity, eccentricity, circularity (5 features)\n        n_shape_features = train_features_df.shape[1] \n        scaler = StandardScaler()\n        train_features = scaler.fit_transform(train_features_df)\n        val_features = scaler.transform(val_features_df)\n        test_features = scaler.transform(test_features_df)\n        \n        # --- 7.3 Prepare Data and Datasets ---\n        train_transform = A.Compose([\n            A.Resize(height=IMAGE_SIZE[0], width=IMAGE_SIZE[1]), A.HorizontalFlip(p=0.5),\n            A.ShiftScaleRotate(shift_limit=0.05, scale_limit=0.05, rotate_limit=10, p=0.5),\n            A.RandomBrightnessContrast(p=0.3), A.GaussNoise(p=0.2), A.OpticalDistortion(p=0.2),\n            A.CoarseDropout(max_holes=8, max_height=8, max_width=8, p=0.2),\n            A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]), ToTensorV2()\n        ])\n        eval_transform = A.Compose([\n            A.Resize(height=IMAGE_SIZE[0], width=IMAGE_SIZE[1]), \n            A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]), ToTensorV2()\n        ])\n        minority_transform = A.Compose([ # More aggressive augmentation\n            A.Resize(height=IMAGE_SIZE[0], width=IMAGE_SIZE[1]), A.HorizontalFlip(p=0.8), A.VerticalFlip(p=0.5),\n            A.ShiftScaleRotate(shift_limit=0.1, scale_limit=0.1, rotate_limit=20, p=0.8),\n            A.RandomBrightnessContrast(brightness_limit=0.3, contrast_limit=0.3, p=0.5),\n            A.GaussNoise(p=0.3), A.OpticalDistortion(p=0.3),\n            A.CoarseDropout(max_holes=16, max_height=16, max_width=16, p=0.3),\n            A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]), ToTensorV2()\n        ])\n\n        class HybridDataset(Dataset):\n            def __init__(self, dataframe, pred_masks, shape_features, transform, minority_classes=None, minority_transform=None):\n                self.df = dataframe.reset_index(drop=True)\n                self.masks = pred_masks\n                self.features = shape_features\n                self.transform = transform\n                self.minority_classes = minority_classes if minority_classes else []\n                self.minority_transform = minority_transform if minority_transform else transform\n\n            def __len__(self): return len(self.df)\n\n            def __getitem__(self, idx):\n                row = self.df.iloc[idx]\n                image = cv2.imread(row['image_file_path'])\n                image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n                label = row['class_label']\n                \n                # Conditional augmentation\n                if label in self.minority_classes and self.minority_transform:\n                    transformed = self.minority_transform(image=image)\n                else:\n                    transformed = self.transform(image=image)\n\n                img_tensor = transformed['image']\n                \n                # Apply mask to the image (Liver only)\n                # Ensure mask is the right size/shape for element-wise multiplication\n                pred_mask = self.masks[idx].to(img_tensor.device) \n                masked_image = img_tensor * pred_mask\n                \n                shape_feats = torch.tensor(self.features[idx], dtype=torch.float)\n                label = torch.tensor(label, dtype=torch.long)\n                return masked_image, shape_feats, label\n\n        # Use setting from previous best run: only 'Severe' (2) gets strong augmentation\n        minority_classes_list = [2] \n        train_cls_dataset = HybridDataset(\n            train_df, train_pred_masks, train_features, \n            transform=train_transform, minority_classes=minority_classes_list, minority_transform=minority_transform\n        )\n        val_cls_dataset = HybridDataset(val_df, val_pred_masks, val_features, eval_transform)\n        test_cls_dataset = HybridDataset(test_df, test_pred_masks, test_features, eval_transform)\n        \n        # WeightedRandomSampler for class balancing\n        class_counts = train_df['class_label'].value_counts().to_dict()\n        num_samples = len(train_df)\n        class_weights_sampler = {i: num_samples / class_counts.get(i, 1) for i in range(NUM_CLASSES)}\n        sample_weights = np.array([class_weights_sampler[t] for t in train_df['class_label'].values])\n        sampler = torch.utils.data.WeightedRandomSampler(\n            weights=torch.from_numpy(sample_weights).double(), num_samples=len(sample_weights), replacement=True\n        )\n\n        train_cls_loader = DataLoader(train_cls_dataset, batch_size=BATCH_SIZE, sampler=sampler, num_workers=2, pin_memory=True)\n        val_cls_loader = DataLoader(val_cls_dataset, BATCH_SIZE, shuffle=False, num_workers=2, pin_memory=True)\n        test_cls_loader = DataLoader(test_cls_dataset, BATCH_SIZE, shuffle=False, num_workers=2, pin_memory=True)\n        print(\"\\nHybrid datasets and loaders are ready with WeightedRandomSampler. ğŸ“Š\")\n\n        # --- 7.4 HYBRID MODEL ARCHITECTURE (Dynamic Encoder) ---\n        \n        # PLACEHOLDER MODELS (since external weights/complex definitions are unavailable)\n        class CoAnNetFeatureExtractor(nn.Module):\n            # CoAnNet is complex; we use a simpler block with the expected output dimension\n            def __init__(self):\n                super().__init__()\n                self.conv = nn.Conv2d(3, 512, kernel_size=3, padding=1)\n                self.pool = nn.AdaptiveAvgPool2d((1, 1))\n                self.identity = nn.Identity()\n            def forward(self, x):\n                x = F.relu(self.conv(x))\n                return self.identity(self.pool(x).flatten(1))\n\n        class SimCLRResNet50(nn.Module):\n            # SimCLR is ResNet50 with special weights; we use a standard pre-trained one\n            def __init__(self):\n                super().__init__()\n                self.base = models.resnet50(weights=models.ResNet50_Weights.IMAGENET1K_V1)\n                self.base.fc = nn.Identity()\n            def forward(self, x):\n                return self.base(x)\n\n\n        class HybridClassifier(nn.Module):\n            def __init__(self, cnn_name: str, n_classes=3, n_shape_features=5):\n                super().__init__()\n                \n                cnn_feature_dim = 0\n                \n                if cnn_name == 'resnet34':\n                    self.cnn_base = models.resnet34(weights=models.ResNet34_Weights.DEFAULT)\n                    cnn_feature_dim = self.cnn_base.fc.in_features\n                    self.cnn_base.fc = nn.Identity()\n                elif cnn_name == 'resnet50' or cnn_name == 'ResNet50_GAP':\n                    # ResNet50 and ResNet50_GAP are the same in feature extraction layer, GAP is handled by AdaptiveAvgPool2d.\n                    self.cnn_base = models.resnet50(weights=models.ResNet50_Weights.DEFAULT)\n                    cnn_feature_dim = self.cnn_base.fc.in_features\n                    self.cnn_base.fc = nn.Identity()\n                elif cnn_name == 'densenet121':\n                    self.cnn_base = models.densenet121(weights=models.DenseNet121_Weights.DEFAULT)\n                    cnn_feature_dim = self.cnn_base.classifier.in_features\n                    self.cnn_base.classifier = nn.Identity()\n                elif cnn_name == 'EfficientNet_B0':\n                    self.cnn_base = models.efficientnet_b0(weights=models.EfficientNet_B0_Weights.DEFAULT)\n                    cnn_feature_dim = self.cnn_base.classifier[1].in_features\n                    self.cnn_base.classifier = nn.Identity() # Remove the default classifier\n                elif cnn_name == 'SimCLR_ResNet50':\n                    self.cnn_base = SimCLRResNet50() # Custom/Pre-loaded weights are required for true SimCLR\n                    cnn_feature_dim = 2048\n                elif cnn_name == 'CoAnNet':\n                    self.cnn_base = CoAnNetFeatureExtractor() # Placeholder\n                    cnn_feature_dim = 512 # Placeholder dimension\n                else:\n                    raise ValueError(f\"Unknown CNN name: {cnn_name}\")\n                    \n                print(f\"Loaded {cnn_name} encoder (Dim: {cnn_feature_dim})\")\n\n                self.tabular_mlp = nn.Sequential(\n                    nn.Linear(n_shape_features, 64), nn.ReLU(), nn.Dropout(0.4),\n                    nn.Linear(64, 32), nn.ReLU(), nn.Dropout(0.4)\n                )\n                self.fusion_classifier = nn.Sequential(\n                    nn.Linear(cnn_feature_dim + 32, 256), nn.ReLU(), nn.Dropout(0.5),\n                    nn.Linear(256, n_classes)\n                )\n                \n            def forward(self, image, shape_features):\n                cnn_features = self.cnn_base(image)\n                # Ensure 1D output after CNN\n                if cnn_features.ndim > 2:\n                    # Adaptive Avg Pool for any remaining spatial dimensions (e.g., in ResNet/EfficientNet backbone output)\n                    cnn_features = F.adaptive_avg_pool2d(cnn_features, (1, 1)).flatten(1) \n                    \n                tabular_features = self.tabular_mlp(shape_features)\n                combined_features = torch.cat([cnn_features, tabular_features], dim=1)\n                output = self.fusion_classifier(combined_features)\n                return output\n\n        # --- 7.5 TRAIN AND EVALUATE FUNCTION PER MODEL ---\n        # *** UPDATED HERE: Added 'model_id' for unique saving/logging ***\n        def train_and_evaluate_single_model(model_id: str, cnn_name: str, train_loader, val_loader, test_loader, class_weights, n_shape_features, device, num_epochs=NUM_EPOCHS, patience=10):\n            \"\"\"Trains and evaluates a single hybrid model.\"\"\"\n            print(f\"\\n=============================================\")\n            print(f\"ğŸš€ Starting Training for Model: {model_id} (Arch: {cnn_name})\")\n            print(f\"=============================================\")\n            \n            cls_model = HybridClassifier(cnn_name=cnn_name, n_classes=NUM_CLASSES, n_shape_features=n_shape_features).to(device)\n            \n            # Use gamma=2.0 as it gave the best result so far\n            criterion = FocalLoss(alpha=class_weights, gamma=2.0)\n            \n            optimizer = torch.optim.AdamW(cls_model.parameters(), lr=1e-4, weight_decay=1e-5)\n            scheduler = ReduceLROnPlateau(optimizer, mode='max', factor=0.2, patience=5, verbose=False)\n            \n            epochs_no_improve = 0; best_val_accuracy = 0.0\n            model_save_path = f'best_classifier_model_{model_id}.pth' # Use unique model_id\n            \n            for epoch in range(num_epochs):\n                cls_model.train()\n                running_loss = 0.0\n                for images, shapes, labels in train_loader:\n                    images, shapes, labels = images.to(device), shapes.to(device), labels.to(device)\n                    optimizer.zero_grad()\n                    outputs = cls_model(images, shapes)\n                    loss = criterion(outputs, labels); loss.backward(); optimizer.step()\n                    running_loss += loss.item()\n                \n                cls_model.eval()\n                val_corrects = 0\n                with torch.no_grad():\n                    for images, shapes, labels in val_loader:\n                        images, shapes, labels = images.to(device), shapes.to(device), labels.to(device)\n                        outputs = cls_model(images, shapes); _, preds = torch.max(outputs, 1)\n                        val_corrects += torch.sum(preds == labels.data)\n\n                val_accuracy = val_corrects.double() / len(val_loader.dataset)\n                scheduler.step(val_accuracy)\n\n                # Early stopping logic\n                if val_accuracy > best_val_accuracy:\n                    best_val_accuracy = val_accuracy\n                    torch.save(cls_model.state_dict(), model_save_path)\n                    epochs_no_improve = 0\n                else:\n                    epochs_no_improve += 1\n                    \n                if epochs_no_improve >= patience:\n                    print(f\"â¹ï¸ Early stopping triggered for {model_id} at epoch {epoch+1}.\")\n                    break\n                    \n            print(f\"Final Best Validation Accuracy for {model_id}: {best_val_accuracy:.4f}\")\n\n            # Final Test Evaluation\n            try:\n                cls_model.load_state_dict(torch.load(model_save_path, map_location=device))\n            except Exception as e:\n                 print(f\"âš ï¸ Could not load best weights for {model_id}. Using current weights for testing. Error: {e}\")\n            cls_model.eval()\n            all_labels, all_preds = [], []\n            with torch.no_grad():\n                for images, shapes, labels in test_loader:\n                    images, shapes, labels = images.to(device), shapes.to(device), labels.to(device)\n                    outputs = cls_model(images, shapes); _, preds = torch.max(outputs, 1)\n                    all_labels.extend(labels.cpu().numpy()); all_preds.extend(preds.cpu().numpy())\n            \n            report = classification_report(all_labels, all_preds, target_names=CLASS_NAMES, zero_division=0, output_dict=True)\n            return np.array(all_preds), report\n\n        # --- 7.6 MASTER EXECUTION AND COMPARISON ---\n        # *** UPDATED HERE: Create experiment list and loop through it ***\n        \n        # List of CNN backbones to test\n        cnn_model_list = [\n            'resnet34', \n            'resnet50', \n            'densenet121', \n            'EfficientNet_B0',\n            'ResNet50_GAP',      # Standard ResNet50 encoder using GAP\n            'SimCLR_ResNet50',   # ResNet50 with placeholder SimCLR weights\n            'CoAnNet'            # Placeholder architecture\n        ]\n        \n        # Define the experiments to run\n        experiments = []\n        for cnn_name in cnn_model_list:\n            # Experiment 1: Conservative Boost (1.2)\n            experiments.append({\n                'model_id': f'{cnn_name}_boost_1.2',\n                'cnn_name': cnn_name,\n                'weights': class_weights_boost_1_2\n            })\n            # Experiment 2: Aggressive Boost (1.5)\n            experiments.append({\n                'model_id': f'{cnn_name}_boost_1.5',\n                'cnn_name': cnn_name,\n                'weights': class_weights_boost_1_5\n            })\n                \n        results = {} \n        for exp in experiments:\n            preds, report = train_and_evaluate_single_model(\n                model_id=exp['model_id'],\n                cnn_name=exp['cnn_name'],\n                train_loader=train_cls_loader, \n                val_loader=val_cls_loader, \n                test_loader=test_cls_loader,\n                class_weights=exp['weights'],  # Pass the correct weights for this experiment\n                n_shape_features=n_shape_features, \n                device=device\n            )\n            # Store results using the unique model_id as the key\n            results[exp['model_id']] = {'preds': preds, 'report': report}\n\n        # --- 7.7 FINAL ENSEMBLE SELECTION AND REPORT ---\n        # *** NO CHANGES NEEDED HERE ***\n        # This section will automatically compare all 14 models in the 'results' dict\n        # and pick the best one for each class.\n        \n        print(\"\\n=============================================\")\n        print(\"ğŸ† FINAL RESULT SELECTION (PER-CLASS BEST RECALL)\")\n        print(\"=============================================\")\n\n        # 1. Get ground truth labels\n        all_labels = np.concatenate([labels.cpu().numpy() for _, _, labels in test_cls_loader])\n        final_preds = np.zeros_like(all_labels)\n        \n        best_model_per_class = {}\n        \n        # 2. Determine the best model for each class based on 'recall' (accuracy per class)\n        comparison_data = []\n        for i, class_name in enumerate(CLASS_NAMES):\n            best_recall = -1.0\n            best_model = None\n            \n            row = {'Class': class_name}\n            for model_name, res in results.items():\n                recall = res['report'].get(class_name, {}).get('recall', 0.0) # Handle missing class gracefully\n                row[model_name] = f'{recall:.4f}'\n                \n                if recall > best_recall:\n                    best_recall = recall\n                    best_model = model_name\n                    \n            best_model_per_class[i] = best_model\n            row['Best Model'] = best_model\n            row['Best Recall'] = f'{best_recall:.4f}'\n            comparison_data.append(row)\n            \n            # 3. Create the final combined prediction array (Per-Class Best Model Selection)\n            class_indices = np.where(all_labels == i)[0]\n            if best_model:\n                 final_preds[class_indices] = results[best_model]['preds'][class_indices]\n\n        # Print comparison table\n        comparison_df = pd.DataFrame(comparison_data)\n        print(\"\\n--- PER-CLASS RECALL COMPARISON ---\")\n        print(comparison_df.to_markdown(index=False))\n        \n        # Print Final Combined Report\n        print(\"\\n--- FINAL CLASSIFICATION REPORT (Combined Best Recall) ---\")\n        print(classification_report(all_labels, final_preds, target_names=CLASS_NAMES, zero_division=0))\n\n        # Confusion Matrix for the Combined Result\n        cm_final = confusion_matrix(all_labels, final_preds, labels=range(NUM_CLASSES))\n        disp_final = ConfusionMatrixDisplay(confusion_matrix=cm_final, display_labels=CLASS_NAMES)\n        fig, ax = plt.subplots(figsize=(8, 8))\n        disp_final.plot(ax=ax, cmap=plt.cm.Greens)\n        plt.title(\"Confusion Matrix (Best Per-Class Selection)\")\n        plt.show()\n\n    except Exception as e:\n        print(f\"\\nAn error occurred in the Classification Pipeline: {e}\")\n        import traceback\n        traceback.print_exc()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-12T09:58:16.989030Z","iopub.execute_input":"2025-11-12T09:58:16.989528Z"}},"outputs":[{"name":"stdout","text":"Using device: cuda âš™ï¸\n\n--- Creating DataFrames ---\nSuccessfully created dataframes: 5364 train, 674 val, 664 test.\nOriginal calculated weights: [0.82055989 0.8641856  1.60215054]\nWeights (Boost 1.2): tensor([0.8206, 1.0370, 1.6022], device='cuda:0')\nWeights (Boost 1.5): tensor([0.8206, 1.2963, 1.6022], device='cuda:0')\n\n--- Loading segmentation models... ---\nSegmentation models loaded successfully! âœ…\n\n\n--- Starting Classification Stage with Hybrid Model ---\n\nGenerating ensemble segmentation predictions...\n","output_type":"stream"},{"name":"stderr","text":"Generating Train Masks:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 251/336 [02:18<00:53,  1.57it/s]","output_type":"stream"}],"execution_count":null}]}